{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import textwrap as tw\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from ast import literal_eval\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "from emv.settings import DRIVE_PATH\n",
    "\n",
    "# Clustering\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# DR\n",
    "from umap import UMAP\n",
    "from umap.umap_ import nearest_neighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from trimap import TRIMAP\n",
    "\n",
    "# Metrics\n",
    "from emv.embeddings.dr_eval import compute_embeddings, compute_umap_embeddings, plot_embeddings, format_params, plot_embeddings_with_images\n",
    "from emv.embeddings.dr_eval import \\\n",
    "    compute_coranking_metrics, \\\n",
    "    random_triplet_accuracy, \\\n",
    "    compute_pcc, \\\n",
    "    global_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Imagenet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FOLDER = DRIVE_PATH + \"rts/aibox-vectors/\"\n",
    "rts_videos = pd.read_csv(DRIVE_PATH + \"rts/aibox-vectors/videos.csv\")\n",
    "rts_videos[\"path\"] = rts_videos.id_result.map(lambda x: SAMPLE_FOLDER + \"videos/\" + x[-1] + \"/\" + x[-2] + \"/\" + x[-3] + \"/\" + x + \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_features_per_video = {}\n",
    "for umid,video in tqdm(zip(rts_videos.umid, rts_videos.path)):\n",
    "    npz = np.load(video + \"features_mean.npz\", allow_pickle=True)\n",
    "    imagenet_features_per_video[umid] = {item: npz[item] for item in npz.files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_videos[\"imagenet_features\"] = rts_videos.umid.map(lambda x: imagenet_features_per_video[x])\n",
    "rts_videos[\"scenes_tc\"] = rts_videos.imagenet_features.map(lambda x: x.get(\"scenes_tc\"))\n",
    "rts_videos[\"imagenet_features\"] = rts_videos.imagenet_features.map(lambda x: x.get(\"imagenet_features_mean\"))\n",
    "rts_videos[\"scenes_ids\"] = rts_videos.scenes_tc.map(lambda x: [i for i,w in enumerate(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_videos = rts_videos.explode([\"imagenet_features\", \"scenes_tc\", \"scenes_ids\"]).reset_index(drop=True)\n",
    "rts_videos.dropna(inplace=True)\n",
    "rts_videos[\"scenes_length\"] = rts_videos.scenes_tc.map(lambda x: x[1] - x[0])\n",
    "rts_videos[\"thumbnail\"] = rts_videos.apply(lambda df: df[\"path\"] + \"ims_scene/\" + str(df[\"scenes_ids\"]) + \".jpg\", axis = 1)\n",
    "rts_videos = rts_videos[rts_videos.scenes_length > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_videos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_videos = rts_videos.sample(10000, random_state=42)\n",
    "features = np.array(rts_videos.imagenet_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA embeddings\n",
    "embeddings_results = [compute_embeddings(features = features, reducer = PCA, params = {\"n_components\": 2})]\n",
    "\n",
    "# UMAP embeddings\n",
    "n_neighbors = [50, 100, 500]\n",
    "embeddings_results.extend(compute_umap_embeddings(features = features, n_neighbors = n_neighbors))\n",
    "\n",
    "# TSNE embeddings\n",
    "perps = [5, 10, 50, 100]\n",
    "for perp in perps:\n",
    "    embeddings_results.append(compute_embeddings(features = features, reducer = TSNE, params = {\"n_components\": 2, \"metric\": \"cosine\", \"perplexity\": perp}))\n",
    "    \n",
    "# TRIMAP embeddings\n",
    "n_inliers_values = [10, 20, 50] # Ratio of 2:1:1 for n_inliers:n_outliers:n_random (as recommended in the paper)\n",
    "for n in n_inliers_values:\n",
    "    m = int(0.5 * n)\n",
    "    embeddings_results.append(compute_embeddings(features = features, reducer = TRIMAP, params = {\"n_inliers\": n, \"n_outliers\": m, \"n_random\": m, \"distance\": \"cosine\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embeddings(embeddings_results, \"Embeddings on the RTS sample imagenet features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a maximum of 1000 thumbnails on the plot\n",
    "thumbnails = rts_videos.thumbnail.tolist()\n",
    "EVERY_N = int(len(thumbnails) / 1000) \n",
    "if EVERY_N < 1:\n",
    "    EVERY_N = 1\n",
    "    \n",
    "thumbnails = [Image.open(thumbnail) for thumbnail in thumbnails[::EVERY_N]]\n",
    "embeddings = embeddings_results[-1][\"embeddings\"][::EVERY_N]\n",
    "\n",
    "plot_embeddings_with_images(embeddings, thumbnails, zoom = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Random Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_d = squareform(pdist(features, metric=\"euclidean\"))\n",
    "dists,knn = NearestNeighbors(n_neighbors=len(features) - 1).fit(features).kneighbors()\n",
    "\n",
    "for result in tqdm(embeddings_results):\n",
    "    embeddings_d = squareform(pdist(result[\"embeddings\"], metric=\"euclidean\"))\n",
    "    result[\"triplet_accuracy_local\"] = random_triplet_accuracy(knn, original_d, embeddings_d, sampling = \"local\", n_repetitions=100)\n",
    "    result[\"triplet_accuracy_mixed\"] = random_triplet_accuracy(knn, original_d, embeddings_d, sampling = \"mixed\", n_repetitions=100)\n",
    "    result[\"triplet_accuracy_global\"] = random_triplet_accuracy(knn, original_d, embeddings_d, sampling = \"global\", n_repetitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [tw.fill(f\"{result['reducer'].__name__} - {format_params(result['reducer_params'])}\", width = 20) for result in embeddings_results]\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 15))\n",
    "for i, sampling in enumerate([\"local\", \"mixed\", \"global\"]):\n",
    "    for j, result in enumerate(embeddings_results):\n",
    "        acc, std = result[f\"triplet_accuracy_{sampling}\"]\n",
    "        axs[i].errorbar(j, acc, yerr = std, fmt = \"o\", color = \"black\")\n",
    "    axs[i].set_ylabel(\"Accuracy\")\n",
    "    axs[i].set_title(f\"Random triplet accuracy ({sampling} sampling)\")\n",
    "    axs[i].set_xticks(range(len(embeddings_results)), labels, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in tqdm(embeddings_results):\n",
    "    result[\"global_score\"] = global_score(features, result[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for i,result in enumerate(embeddings_results):\n",
    "    plt.bar(i, result[\"global_score\"], color = \"black\")\n",
    "plt.xticks(range(len(embeddings_results)), [tw.fill(f\"{result['reducer'].__name__} - {format_params(result['reducer_params'])}\", width = 20) for result in embeddings_results], rotation=0)\n",
    "plt.ylabel(\"GS\")\n",
    "plt.title(\"Global Score (GS) of the embeddings\")\n",
    "plt.ylim(0.875, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emv-requDRed-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
