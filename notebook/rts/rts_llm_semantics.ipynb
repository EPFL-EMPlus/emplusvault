{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d11e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d48013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emv.db.dao import DataAccessObject\n",
    "from emv.db.queries import get_features_by_type_paginated, count_features_by_type\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from emv.api.models import Feature\n",
    "from emv.api.models import Projection, MapProjectionFeatureCreate\n",
    "from emv.db.queries import create_projection, create_map_projection_feature, create_feature, update_feature, count_media_by_library_id\n",
    "from emv.io.media import create_square_atlases\n",
    "from umap import UMAP\n",
    "import numba\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sqlalchemy.sql import text\n",
    "from datetime import datetime\n",
    "import textwrap as tw\n",
    "\n",
    "from emv.db.queries import get_all_media_by_library_id, get_library_id_from_name, get_library_from_name, check_media_exists, get_media_by_id, delete_feature_by_type\n",
    "from emv.storage.storage import get_storage_client\n",
    "from emv.features.image import embed_images\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a12d0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a3ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = count_features_by_type(\"transcript+ner\", short_clips_only=True)\n",
    "print(f\"Total features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9758149",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = total_features + 1\n",
    "data = get_features_by_type_paginated(\"transcript+ner\", page_size=10000, short_clips_only=True)\n",
    "\n",
    "for _ in tqdm(range(MAX_FEATURES // 10000)):\n",
    "    last_seen_id = data[-1].get(\"feature_id\", None)\n",
    "    if last_seen_id is None:\n",
    "        break\n",
    "    data.extend(get_features_by_type_paginated(\"transcript+ner\", page_size=10000, last_seen_feature_id=last_seen_id, short_clips_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop fields not needed\n",
    "df = []\n",
    "for d in tqdm(data):\n",
    "    df.append(\n",
    "        {\n",
    "            \"media_id\": d[\"media_id\"],\n",
    "            \"data\": d[\"data\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "df = pd.DataFrame(df)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "print(f\"Retrieved {len(df)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34922462",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = count_features_by_type(\"locations\", short_clips_only=True) + 1\n",
    "PAGE_SIZE = 10000\n",
    "features = get_features_by_type_paginated(\"locations\", page_size=PAGE_SIZE)\n",
    "\n",
    "for _ in tqdm(range(MAX_FEATURES // PAGE_SIZE)):\n",
    "    last_seen_id = features[-1].get(\"feature_id\", None)\n",
    "    if last_seen_id is None:\n",
    "        break\n",
    "    features.extend(get_features_by_type_paginated(\"locations\", page_size=PAGE_SIZE, last_seen_feature_id=last_seen_id))\n",
    "    \n",
    "features = pd.DataFrame(features)\n",
    "print(f\"Retrieved {len(features)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4609b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"data\": \"transcript_data\"}, inplace=True)\n",
    "features = features.merge(df, on=\"media_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"transcript\"] = features[\"transcript_data\"].map(lambda x: x.get(\"transcript\", None))\n",
    "features[\"entities\"] = features[\"transcript_data\"].map(lambda x: x.get(\"entities\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521094b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"data\"] = features.apply(lambda row: {**row[\"data\"], \"transcript\": row[\"transcript\"]}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f788799",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"tabularisai/multilingual-sentiment-analysis\",\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "]\n",
    "pipe = pipeline(\"text-classification\", model=models[3], return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in features.transcript.values[:10]:\n",
    "    if t is None:\n",
    "        continue\n",
    "    try:\n",
    "        result = pipe(t)\n",
    "        print(result)\n",
    "        print(tw.fill(t, 100))\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing transcript: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef492cec",
   "metadata": {},
   "source": [
    "## Theme classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f55138",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_possibles = [\n",
    "    \"Information & Actualité\",\n",
    "    \"Débats & Talk Shows\",\n",
    "    \"Société & Monde\",\n",
    "    \"Culture & Connaissance\",\n",
    "    \"Arts & Spectacles\",\n",
    "    \"Musique\",\n",
    "    \"Sport\",\n",
    "    \"Fiction & Divertissement\",\n",
    "    \"Jeunesse\",\n",
    "    \"Religion & Spiritualité\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = features.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"theme_llm\"] = sample.transcript.map(lambda x: classifier(x, themes_possibles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"theme\"] = sample[\"theme_llm\"].map(lambda x: x[\"labels\"][0])\n",
    "sample[\"theme_score\"] = sample[\"theme_llm\"].map(lambda x: x[\"scores\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b53404",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.theme.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1aa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.theme_score.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f24a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[sample.theme_score > 0.8].theme.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b24fa",
   "metadata": {},
   "source": [
    "# IMI Ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6922206",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = count_features_by_type(\"locations\", short_clips_only=True) + 1\n",
    "PAGE_SIZE = 10000\n",
    "features = get_features_by_type_paginated(\"locations\", page_size=PAGE_SIZE)\n",
    "\n",
    "for _ in tqdm(range(MAX_FEATURES // PAGE_SIZE)):\n",
    "    last_seen_id = features[-1].get(\"feature_id\", None)\n",
    "    if last_seen_id is None:\n",
    "        break\n",
    "    features.extend(get_features_by_type_paginated(\"locations\", page_size=PAGE_SIZE, last_seen_feature_id=last_seen_id))\n",
    "    \n",
    "features = pd.DataFrame(features)\n",
    "print(f\"Retrieved {len(features)} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rts_metadata = pd.read_hdf(\"data/rts_metadata.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"original_id\"] = features[\"media_id\"].map(lambda x: x.split(\"-\")[1])\n",
    "features = features.merge(rts_metadata, left_on=\"original_id\", right_on=\"mediaId\", how=\"left\")\n",
    "features[\"date\"] = features[\"publishedDate\"].map(lambda x: datetime.strptime(x.split(\"T\")[0], \"%Y-%m-%d\") if x is not None else None)\n",
    "features[\"year\"] = features[\"date\"].map(lambda x: x.year)\n",
    "\n",
    "features[\"year\"].value_counts().sort_index().plot(kind=\"bar\", figsize=(10, 6), title=\"Number of features per year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52242be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop_duplicates(subset=[\"feature_id\"], inplace=True)\n",
    "print(f\"Number of features after dropping duplicates: {len(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"transcript\"] = features[\"data\"].map(lambda x: x.get(\"transcript\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3fa889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read jsonl file\n",
    "with open(\"data/locations_ontologies_all.jsonl\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "data = [json.loads(line) for line in data]\n",
    "data = pd.DataFrame(data)\n",
    "data.drop(columns=[\"media_id\"], inplace=True)\n",
    "print(f\"Retrieved {len(data)} instances of LLM semantics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the \"data\" column\n",
    "for key in data[\"ontology\"][0].keys():\n",
    "    data[key] = data[\"ontology\"].map(lambda x: x[key])\n",
    "data = data.drop(columns=[\"ontology\"])\n",
    "print(f\"Retrieved {len(data)} instances of LLM semantics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf899579",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.merge(data, left_on=\"feature_id\", right_on=\"transcript_id\", how=\"left\")\n",
    "print(f\"Retrieved {len(features)} instances with LLM semantics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"categories\"].fillna(\"[]\", inplace=True)\n",
    "features[\"categories\"] = features[\"categories\"].map(lambda x: literal_eval(x) if isinstance(x, str) else x)\n",
    "features[\"events\"].fillna(\"[]\", inplace=True)\n",
    "features[\"events\"] = features[\"events\"].map(lambda x: literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"keywords\"] = features.events.map(lambda x: [e.get(\"keywords\", []) for e in x])\n",
    "features[\"keywords\"] = features.keywords.map(lambda x: [item for sublist in x for item in sublist])\n",
    "features[\"keywords\"] = features.keywords.map(lambda x: list(set(x)))\n",
    "features[\"keywords\"] = features.keywords.map(lambda x: [k for k in x if k is not None and k != \"\"])\n",
    "\n",
    "# Remove location names from keywords\n",
    "with open(\"emv/features/cities.json\", \"r\") as f:\n",
    "    cities = json.load(f)\n",
    "    \n",
    "cities = list(cities.keys()) + [\"Switzerland\", \"Geneva\", \"Bern\"]\n",
    "features[\"keywords\"] = features.keywords.map(lambda x: [k for k in x if k not in cities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23555c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"publishedDate\"] = features[\"publishedDate\"].map(lambda x: x.split(\"T\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"new_feature_id\"] = features.apply(lambda x: update_feature(x[\"feature_id\"],\n",
    "                                                                      Feature(\n",
    "                                                                          feature_type=\"locations\",\n",
    "                                                                          version=\"1.3\",\n",
    "                                                                          model_name='transcript+ner+geolocation',\n",
    "                                                                          model_params={},\n",
    "                                                                          data={\n",
    "                                                                            \"location\": x[\"data\"][\"location\"],\n",
    "                                                                            \"geo_coords\": x[\"data\"][\"geo_coords\"],\n",
    "                                                                            \"media_path\": x[\"data\"][\"media_path\"],\n",
    "                                                                            \"transcript\": x[\"data\"][\"transcript\"],\n",
    "                                                                            \"categories\": x[\"categories\"],\n",
    "                                                                            \"keywords\": x[\"keywords\"],\n",
    "                                                                            \"date\": x[\"publishedDate\"]\n",
    "                                                                              },\n",
    "                                                                          media_id=x['media_id']\n",
    "                                                                      ))[\"feature_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a8797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emv-requDRed-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
