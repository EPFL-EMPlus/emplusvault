{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "\n",
    "from emv.client.get_content import get_features\n",
    "from emv.utils import dataframe_from_hdf5\n",
    "from emv.settings import DRIVE_PATH\n",
    "\n",
    "from emv.features.wikidata import get_wikidata_id, get_property, get_wikidata_label\n",
    "from emv.features.wikidata import process_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(feature_type='transcript+ner', max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = dataframe_from_hdf5(DRIVE_PATH + \"rts/metadata\", \"rts_metadata\")\n",
    "metadata.reset_index(inplace=True)\n",
    "metadata.rename(columns = {\"mediaId\": \"rts_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rts_id\"] = df[\"media_id\"].apply(lambda x: x.split(\"-\")[1])\n",
    "df = df.merge(metadata, on='rts_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rts = pd.read_csv(DRIVE_PATH + \"rts/aibox-vectors/videos.csv\")\n",
    "sample_rts_ids = sample_rts.umid.tolist()\n",
    "df = df[df.rts_id.isin(sample_rts.umid)].reset_index(drop=True)\n",
    "print(f\"Processed {len(df.rts_id.unique())} videos out of {len(sample_rts_ids)} - {len(df.rts_id.unique())/len(sample_rts_ids)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "df[\"year\"] = df[\"publishedDate\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(data):\n",
    "    entities = []\n",
    "    if \"entities\" in data.keys():\n",
    "        entities = data[\"entities\"]\n",
    "    else:\n",
    "        entities = [t.get(\"entities\", []) for t in data.get(\"transcript\", [])]\n",
    "        entities = [e for sublist in entities for e in sublist]\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entities\"] = df[\"data\"].apply(get_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types = list(set([e[1] for sublist in df.entities for e in sublist]))\n",
    "entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"locations\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"LOC\" and len(e[0]) > 2])\n",
    "df[\"people\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"PER\" and len(e[0]) > 2])\n",
    "df[\"orgs\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"ORG\" and len(e[0]) > 1])\n",
    "df[\"misc\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"MISC\" and len(e[0]) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"media_id\", \"rts_id\", \"year\", \"data\", \"categoryName\", \"contentType\", \"title\", \"resume\", \"mediaDuration\", \"locations\", \"people\", \"orgs\", \"misc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/rts_sample.csv\", index=False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load presaved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/rts_sample.csv\", \n",
    "                 sep = \"\\t\", \n",
    "                 converters = {\n",
    "                     \"data\": literal_eval,\n",
    "                     \"locations\": literal_eval,\n",
    "                     \"people\": literal_eval,\n",
    "                     \"orgs\": literal_eval,\n",
    "                     \"misc\": literal_eval\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transcript\"] = df[\"data\"].map(lambda x: x.get(\"transcript\", []))\n",
    "df = df[df.transcript.map(lambda x: type(x) == list)].reset_index(drop=True) # Get full videos with speaker diarization info\n",
    "print(f\"Processed {len(df)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transcript.map(lambda x: len(x)).mean(), df.transcript.map(lambda x: len(x)).sem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transcript.map(lambda x: len(x)).hist(bins=100, grid = False, edgecolor = 'black')\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.transcript.map(lambda x: \" \".join([t[\"t\"] for t in x])).sum().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mediaDuration.sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Locations extracted: {df.locations.explode().nunique()}\")   \n",
    "print(f\"People extracted: {df.people.explode().nunique()}\")\n",
    "print(f\"Organizations extracted: {df.orgs.explode().nunique()}\")\n",
    "print(f\"Miscellaneous entities extracted: {df.misc.explode().nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Locations\", \"People\", \"Organizations\", \"Miscellaneous\"]\n",
    "sizes = [df.locations.explode().nunique(), df.people.explode().nunique(), df.orgs.explode().nunique(), df.misc.explode().nunique()]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, sizes, edgecolor = 'black')\n",
    "plt.ylabel(\"Number of entities\", fontweight = \"bold\")\n",
    "plt.xlabel(\"Entity type\", fontweight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entities extracted: {sum(sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4)) \n",
    "df.year.hist(bins=50, grid = False, edgecolor='black')\n",
    "plt.xlabel(\"Year\", fontweight='bold')\n",
    "plt.ylabel(\"Number of videos\", fontweight='bold')\n",
    "plt.savefig(\"data/outputs/year_distribution.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map of Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[[\"locations\", \"year\"]].explode(\"locations\").dropna().groupby(\"locations\").agg(list)\n",
    "locations[\"count\"] = locations[\"year\"].apply(len)\n",
    "locations[\"year\"] = locations.year.map(lambda x: Counter(x))\n",
    "locations = locations.sort_values(\"count\", ascending=False).reset_index()\n",
    "locations.head(10)\n",
    "\n",
    "print(f\"Found {len(locations)} locations in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emv/features/cities.json\", \"r\") as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame([{\"locations\":k, \"lon\":float(v[0]), \"lat\":float(v[1])} for k,v in cities.items() if len(v) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities = pd.merge(locations, cities, on=\"locations\", how=\"left\").dropna(subset = [\"lat\", \"lon\"]).reset_index()\n",
    "found_cities[\"first_mention\"] = found_cities[\"year\"].map(lambda x: min(x.keys()))\n",
    "found_cities = found_cities[found_cities[\"count\"] > 25]\n",
    "found_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities[\"count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branca.colormap as cm\n",
    "\n",
    "colormap = cm.linear.plasma.scale(found_cities[\"first_mention\"].min(), found_cities[\"first_mention\"].max()).to_step(10)\n",
    "\n",
    "# Create a base map centered around Switzerland\n",
    "m = folium.Map(location=[46.8182, 8.2275], zoom_start=8)\n",
    "size_multiplier = 1\n",
    "# Add city points to the map\n",
    "for index, row in found_cities.iterrows():\n",
    "    color = colormap(row['first_mention'])\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lon'], row['lat']),\n",
    "        radius = np.sqrt(row['count'] / np.pi) * size_multiplier,\n",
    "        color=\"black\",\n",
    "        weight = 1,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.6,\n",
    "        tooltip=row['locations'] + ': ' + str(row['count']) + ' occurrences - First mentioned in ' + str(row['first_mention']),\n",
    "    ).add_to(m)\n",
    "\n",
    "m.add_child(colormap)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching with Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations[\"wikidata_id\"] = locations[\"locations\"].apply(get_wikidata_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations[locations.wikidata_id.map(lambda x: len(x) > 0)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations[\"wikidata_label\"] = locations.wikidata_id.map(lambda x: x[0].get(\"label\", \"MISSING_LABEL\") if len(x) > 0 else None)\n",
    "locations[\"wikidata_description\"] = locations.wikidata_id.map(lambda x: x[0].get(\"description\", \"MISSING_DESCRIPTION\") if len(x) > 0 else None)\n",
    "locations[\"wikidata_id\"] = locations.wikidata_id.map(lambda x: x[0].get(\"id\", \"MISSING_ID\") if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_functions = {\n",
    "    'year': 'sum',\n",
    "    'count': 'sum',\n",
    "    'wikidata_id': 'first',\n",
    "    'wikidata_label': 'first',\n",
    "    'wikidata_description': 'first',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_locs = len(locations)\n",
    "locations = locations.groupby(\"wikidata_id\").agg(aggregation_functions).reset_index(drop = True)\n",
    "print(f\"Found {len_locs - len(locations)} duplicate locations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations[locations[\"count\"] > 25]\n",
    "print(f\"Found {len(locations)} locations with more than 25 occurrences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations[\"instance_of\"] = locations.wikidata_id.map(lambda x: get_property(x, \"P31\", delay = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = locations.instance_of.explode().dropna().unique()\n",
    "instances = {id_: get_wikidata_label(id_, delay = 2) for id_ in instances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations[\"instance_of_label\"] = locations.instance_of.map(lambda x: [instances[id_][\"en\"].get(\"value\") for id_ in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.to_csv(\"data/rts_sample_locations_filtered.csv\", index=False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.instance_of_label.explode().value_counts()[:30].plot(kind = \"barh\", figsize=(8, 6))\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"Number of occurrences\", fontweight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv(\"data/rts_sample_locations_filtered.csv\", sep = \"\\t\", converters={\"count\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in locations.instance_of_label.explode().dropna().unique() if \"Switzerland\" in x or \"Swiss\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_swiss_location(instances):\n",
    "    is_swiss_loc = False\n",
    "    for label in [\"municipality of Switzerland\", \"city of Switzerland\", \"cantonal capital of Switzerland\"]:\n",
    "        if label in instances:\n",
    "            is_swiss_loc = True\n",
    "            break\n",
    "    return is_swiss_loc\n",
    "\n",
    "found_cities = locations[locations.instance_of_label.map(is_swiss_location)]\n",
    "print(f\"Found {len(found_cities)} locations in Switzerland.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities[\"coords\"] = found_cities.wikidata_id.map(lambda x: get_property(x, \"P625\"))\n",
    "found_cities[\"lat\"] = found_cities.coords.map(lambda x: float(x[0].replace(\"Point(\", \"\").split(\" \")[0]))\n",
    "found_cities[\"lon\"] = found_cities.coords.map(lambda x: float(x[0].replace(\"Point(\", \"\").split(\" \")[1].replace(\")\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities[\"year\"] = found_cities[\"year\"].map(lambda x: literal_eval(x.replace(\"Counter(\", \"\").replace(\")\", \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities[\"first_mention\"] = found_cities[\"year\"].map(lambda x: min(x.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branca.colormap as cm\n",
    "\n",
    "colormap = cm.linear.plasma.scale(found_cities[\"first_mention\"].min(), found_cities[\"first_mention\"].max()).to_step(10)\n",
    "\n",
    "# Create a base map centered around Switzerland\n",
    "m = folium.Map(location=[46.8182, 8.2275], zoom_start=8.3)\n",
    "size_multiplier = 1.5\n",
    "# Add city points to the map\n",
    "for index, row in found_cities.iterrows():\n",
    "    color = colormap(row['first_mention'])\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lon'], row['lat']),\n",
    "        radius = np.sqrt(row['count'] / np.pi) * size_multiplier,\n",
    "        color=\"black\",\n",
    "        weight = 1,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.6,\n",
    "        tooltip=row['wikidata_label'] + ': ' + str(row['count']) + ' occurrences - First mentioned in ' + str(row['first_mention']),\n",
    "    ).add_to(m)\n",
    "\n",
    "m.add_child(colormap)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mentions_per_year = found_cities.year.sum()\n",
    "n_mentions_per_year = pd.DataFrame(n_mentions_per_year.items(), columns=[\"year\", \"count\"])\n",
    "n_mentions_per_year = n_mentions_per_year.sort_values(\"year\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.lineplot(found_cities[\"year\"])\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = found_cities.year.map(lambda x: min(x.keys())).min()\n",
    "last_year = found_cities.year.map(lambda x: max(x.keys())).max()\n",
    "\n",
    "n_mentions_per_year = found_cities[[\"wikidata_label\", \"year\", \"count\"]].set_index(\"wikidata_label\")\n",
    "n_mentions_per_year[\"year\"] = n_mentions_per_year[\"year\"].map(lambda x: [x.get(year, 0) for year in range(first_year, last_year + 1)])\n",
    "n_mentions_per_year.sort_values(\"count\", ascending=False, inplace=True)\n",
    "n_mentions_per_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.heatmap(n_mentions_per_year[\"year\"].values.tolist(), cmap = \"viridis\")\n",
    "plt.xticks(ticks = np.array(list(range(last_year - first_year + 1))[::5]) + 0.5, labels = range(first_year, last_year + 1, 5), rotation = 0)\n",
    "plt.yticks(ticks = np.arange(len(n_mentions_per_year)) + 0.5, labels = n_mentions_per_year.index, rotation = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i, row in n_mentions_per_year[:10].iterrows():\n",
    "    plt.plot(range(first_year, last_year + 1), row[\"year\"], label = i)\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mentions_per_year_dict = n_mentions_per_year.set_index(\"year\").to_dict()[\"count\"]\n",
    "found_cities[\"relative_counts\"] = found_cities.year.map(lambda x: {k:v / n_mentions_per_year_dict[k] for k,v in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "found_cities[:20].sort_values(\"count\").set_index(\"locations\")[\"count\"].plot(kind=\"barh\")\n",
    "plt.xlabel(\"Number of mentions\")\n",
    "plt.title(\"Top 20 locations mentioned in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 20\n",
    "skip_first_n = 1\n",
    "top_cities = found_cities[skip_first_n:top_N + skip_first_n]\n",
    "counts_per_year = pd.DataFrame(top_cities.year.tolist(), index=top_cities.locations).fillna(0)\n",
    "counts_per_year = counts_per_year.T.sort_index().T\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(counts_per_year, cmap=\"Blues\", cbar_kws={'label': 'Number of mentions'})\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for loc,mentions in counts_per_year.iterrows():\n",
    "    plt.plot(mentions.index, mentions.values, label=loc)\n",
    "plt.legend()\n",
    "plt.title(f\"Number of mentions of the top {len(counts_per_year)} locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 20\n",
    "skip_first_n = 1\n",
    "top_cities = found_cities[skip_first_n:top_N + skip_first_n]\n",
    "counts_per_year = pd.DataFrame(top_cities.relative_counts.tolist(), index=top_cities.locations).fillna(0)\n",
    "counts_per_year = counts_per_year.T.sort_index().T\n",
    "\n",
    "counts_per_year = counts_per_year.div(counts_per_year.sum(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(counts_per_year, cmap=\"Blues\", cbar_kws={'label': 'Number of mentions'})\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for loc,mentions in counts_per_year.iterrows():\n",
    "    plt.plot(mentions.index, mentions.values, label=loc)\n",
    "plt.legend()\n",
    "plt.title(f\"Number of mentions of the top {len(counts_per_year)} locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_people = [\"messieurs\", \"monsieur\", \"madame\", \"mesdames\"]\n",
    "\n",
    "df[\"people\"] = df[\"people\"].apply(lambda x: [p for p in x if p.lower() not in filter_people])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = df[\"transcript\"].to_frame()\n",
    "persons[\"entities\"] = df.transcript.map(lambda x: [t.get(\"entities\", None) for t in x])\n",
    "persons[\"context\"] = df.transcript.map(lambda x: [t[\"t\"] for t in x])\n",
    "persons = persons.explode([\"entities\", \"context\"]).explode(\"entities\").reset_index(drop=True).dropna(subset = [\"entities\"])\n",
    "persons[\"entities\"] = persons.entities.map(lambda x: x[0] if x[1] == \"PER\" else None)\n",
    "persons = persons.dropna(subset = [\"entities\"]).reset_index(drop=True)\n",
    "persons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = df[[\"people\", \"year\"]].explode(\"people\").dropna().groupby(\"people\").agg(list).reset_index()\n",
    "top_persons[\"count\"] = top_persons[\"year\"].apply(len)\n",
    "top_persons[\"year\"] = top_persons.year.map(lambda x: Counter(x))\n",
    "top_persons = top_persons.sort_values(\"count\", ascending=False)\n",
    "print(f\"Found {len(top_persons)} persons in the dataset.\")\n",
    "print(f\"Mean number of mentions per person: {top_persons['count'].mean():.2f} +/- {top_persons['count'].sem():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 50\n",
    "top_persons = top_persons[top_persons[\"count\"] > min_count]\n",
    "print(f\"Found {len(top_persons)} persons with more than {min_count} occurrences.\")\n",
    "top_persons.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"wikidata_search\"] = top_persons[\"people\"].map(lambda x: get_wikidata_id(x, top_n = 10, delay = 1))\n",
    "top_persons = top_persons[top_persons[\"wikidata_search\"].map(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = top_persons[top_persons[\"wikidata_search\"].map(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"wikidata_candidates\"] = top_persons[\"wikidata_search\"].map(lambda x: [(c.get(\"id\"), c.get(\"description\")) for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_candidates = top_persons[[\"people\", \"wikidata_candidates\"]].to_dict(\"records\")\n",
    "persons_candidates[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons[persons.entities == \"Roger\"].head().context.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"wikidata_id\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"id\", \"MISSING_ID\") if len(x) > 0 else None)\n",
    "top_persons[\"wikidata_label\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"label\", \"MISSING_LABEL\") if len(x) > 0 else None)\n",
    "top_persons[\"wikidata_description\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"description\", \"MISSING_DESCRIPTION\") if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_functions = {\n",
    "    'year': 'sum',\n",
    "    'count': 'sum',\n",
    "    'wikidata_id': 'first',\n",
    "    'wikidata_label': 'first',\n",
    "    'wikidata_description': 'first',\n",
    "}\n",
    "\n",
    "len_persons = len(top_persons)\n",
    "top_persons = top_persons.groupby(\"wikidata_id\").agg(aggregation_functions).reset_index(drop = True)\n",
    "print(f\"Found {len_persons - len(top_persons)} duplicate persons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_of = process_batch(top_persons[\"wikidata_id\"].dropna().tolist(), \"P31\", BATCH_SIZE=20)\n",
    "top_persons[\"instance_of\"] = top_persons[\"wikidata_id\"].map(instance_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = top_persons.instance_of.dropna().unique().tolist()\n",
    "instances = {i:get_wikidata_label(i.split(\"/\")[-1]) for i in instances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"instance_of\"] = top_persons[\"instance_of\"].map(instances)\n",
    "top_persons[\"instance_of\"] = top_persons[\"instance_of\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"instance_of\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on humans (instance of Q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons.dropna(subset=[\"wikidata_id\", \"instance_of\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = top_persons[top_persons.instance_of == \"human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = top_persons[\"wikidata_id\"].tolist()\n",
    "citizenship = process_batch(ids, \"P27\", BATCH_SIZE=20)\n",
    "top_persons[\"citizenship\"] = top_persons[\"wikidata_id\"].map(citizenship)\n",
    "\n",
    "occupation = process_batch(ids, \"P106\", BATCH_SIZE=20)\n",
    "top_persons[\"occupation\"] = top_persons[\"wikidata_id\"].map(occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizenship_labels = {k:get_wikidata_label(k.split(\"/\")[-1]) for k in top_persons[\"citizenship\"].dropna().unique()}\n",
    "top_persons[\"citizenship\"] = top_persons[\"citizenship\"].map(citizenship_labels)\n",
    "\n",
    "occupation_labels = {k:get_wikidata_label(k.split(\"/\")[-1]) for k in top_persons[\"occupation\"].dropna().unique()}\n",
    "top_persons[\"occupation\"] = top_persons[\"occupation\"].map(occupation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"citizenship\"] = top_persons[\"citizenship\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")\n",
    "top_persons[\"occupation\"] = top_persons[\"occupation\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "top_persons.occupation.value_counts()[:20].plot(kind=\"barh\", ax=axs[0])\n",
    "axs[0].set_title(\"Occupations of most occurring persons\")\n",
    "top_persons.citizenship.value_counts()[:20].plot(kind=\"barh\", ax=axs[1])\n",
    "axs[1].set_title(\"Citizenships of most occurring persons\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[[\"media_id\", \"rts_id\", \"year\", \"mediaDuration\", \"transcript\"]].explode(\"transcript\")\n",
    "sentences.dropna(subset=\"transcript\", inplace=True)\n",
    "sentences[\"transcript\"] = sentences.transcript.map(lambda x: x.get(\"t\", \"\")).tolist()\n",
    "print(f\"Extracted {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[\"sentence_length\"] = sentences.transcript.map(lambda x: len(x.split()))\n",
    "sentences.sentence_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sentences.year.value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.xticks(range(0, len(sentences.year.value_counts().index), 5), rotation=0)\n",
    "plt.ylabel(\"Number of sentences\")\n",
    "plt.title(\"Number of sentences per year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample N sentences per year\n",
    "N = 100\n",
    "sampled_sentences = pd.DataFrame()\n",
    "for year, group in sentences.groupby(\"year\"):\n",
    "    if N > len(group):\n",
    "        sampled_group = group\n",
    "        print(f\"Only sampled {len(group)} sentences for year {year}\")\n",
    "    else:\n",
    "        sampled_group = group.sample(N)\n",
    "    sampled_sentences = pd.concat([sampled_sentences, sampled_group])\n",
    "sampled_sentences.reset_index(drop=True, inplace=True)\n",
    "print(f\"\\nSampled {len(sampled_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences.transcript.map(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = pipeline(\n",
    "    model = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "    top_k = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"sentiment_scores\"] = sampled_sentences.transcript.map(lambda x: sentiment_classifier(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_score(positive_score, neutral_score, negative_score):\n",
    "    sentiment_score = positive_score - negative_score\n",
    "    return sentiment_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences.dropna(subset=[\"sentiment_scores\"], inplace=True)\n",
    "\n",
    "sampled_sentences[\"sentiment_scores\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: {s[\"label\"]:s[\"score\"] for s in x})\n",
    "sampled_sentences[\"positive_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: x.get(\"positive\", 0))\n",
    "sampled_sentences[\"negative_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: x.get(\"negative\", 0))\n",
    "sampled_sentences[\"neutral_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: x.get(\"neutral\", 0))\n",
    "sampled_sentences[\"top_sentiment\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: max(x, key=x.get))\n",
    "sampled_sentences[\"top_sentiment_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: max(x.values()))\n",
    "\n",
    "def calculate_sentiment_score(positive_score, neutral_score, negative_score):\n",
    "    sentiment_score = positive_score - negative_score\n",
    "    return sentiment_score\n",
    "\n",
    "sampled_sentences[\"sentiment_score\"] = sampled_sentences[[\"positive_score\", \"neutral_score\", \"negative_score\"]].apply(lambda x: calculate_sentiment_score(*x), axis=1)\n",
    "sampled_sentences.drop(columns = [\"sentiment_scores\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sampled_sentences.top_sentiment.value_counts().reindex([\"positive\", \"negative\", \"neutral\"]).plot(kind=\"bar\", ax=axs[0])\n",
    "axs[0].set_title(\"Sentiment distribution\")\n",
    "axs[0].set_xticks(range(3), [\"positive\", \"negative\", \"neutral\"], rotation=0)\n",
    "\n",
    "sampled_sentences.sentiment_score.hist(ax=axs[1], bins=100, grid = False)\n",
    "axs[1].set_title(\"Sentiment score distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top k extreme sentences, by sentiment\n",
    "k = 50\n",
    "print(f\"Top {k} positive sentences\")\n",
    "_ = [print(s) for s in sampled_sentences[sampled_sentences.top_sentiment == \"positive\"].sort_values(\"top_sentiment_score\", ascending=False).transcript.tolist()[:k]]\n",
    "print()\n",
    "print(f\"Top {k} negative sentences\")\n",
    "_ = [print(s) for s in sampled_sentences[sampled_sentences.top_sentiment == \"negative\"].sort_values(\"top_sentiment_score\", ascending=False).transcript.tolist()[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = sampled_sentences.groupby(\"media_id\").sentiment_score.mean()\n",
    "clips = clips.reset_index()\n",
    "clips[\"sentiment_std\"] = sampled_sentences.groupby(\"media_id\").sentiment_score.std().fillna(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].hist(clips.sentiment_score, bins = 100)\n",
    "axs[0].set_title(\"Distribution of the mean sentiment score\")\n",
    "\n",
    "axs[1].hist(clips.sentiment_std, bins = 50)\n",
    "axs[1].set_title(\"Distribution of the standard deviation of the sentiment score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sentiment_scores_per_year = sampled_sentences.groupby(\"year\").sentiment_score.mean().reset_index()\n",
    "sentiment_scores_per_year[\"sentiment_std\"] = sampled_sentences.groupby(\"year\").sentiment_score.std().fillna(0).tolist() / np.sqrt(sampled_sentences.groupby(\"year\").sentiment_score.count().tolist())\n",
    "sentiment_scores_per_year.sort_values(\"year\", inplace=True)\n",
    "\n",
    "plt.errorbar(sentiment_scores_per_year.year, sentiment_scores_per_year.sentiment_score, yerr=sentiment_scores_per_year.sentiment_std, fmt='o')\n",
    "plt.hlines(0, 1949, 2022, color=\"black\", linestyle=\"--\")\n",
    "plt.xlim(1949, 2022)  # Limit the x-axis view\n",
    "plt.ylabel(\"Mean sentiment score\")\n",
    "plt.title(\"Mean sentiment score of sentences per year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[sampled_sentences.year == 1957][[\"transcript\", \"sentiment_score\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"emotions\"] = sampled_sentences.transcript.apply(lambda x: classifier(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"emotions\"] = sampled_sentences[\"emotions\"].map(lambda x: {s[\"label\"]:s[\"score\"] for s in x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"top_emotion\"] = sampled_sentences[\"emotions\"].map(lambda x: max(x, key=x.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"top_emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rts-bWoRmFur-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
