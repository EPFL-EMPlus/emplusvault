{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "\n",
    "from emv.client.get_content import get_features\n",
    "from emv.utils import dataframe_from_hdf5\n",
    "from emv.settings import DRIVE_PATH\n",
    "\n",
    "from emv.features.wikidata import get_wikidata_id, get_property, get_wikidata_label\n",
    "from emv.features.wikidata import process_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(feature_type='transcript+ner', max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = dataframe_from_hdf5(DRIVE_PATH + \"rts/metadata\", \"rts_metadata\")\n",
    "metadata.reset_index(inplace=True)\n",
    "metadata.rename(columns = {\"mediaId\": \"rts_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rts_id\"] = df[\"media_id\"].apply(lambda x: x.split(\"-\")[1])\n",
    "df = df.merge(metadata, on='rts_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rts = pd.read_csv(DRIVE_PATH + \"rts/aibox-vectors/videos.csv\")\n",
    "sample_rts_ids = sample_rts.umid.tolist()\n",
    "df = df[df.rts_id.isin(sample_rts.umid)].reset_index(drop=True)\n",
    "print(f\"Processed {len(df.rts_id.unique())} videos out of {len(sample_rts_ids)} - {len(df.rts_id.unique())/len(sample_rts_ids)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "df[\"year\"] = df[\"publishedDate\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(data):\n",
    "    entities = []\n",
    "    if \"entities\" in data.keys():\n",
    "        entities = data[\"entities\"]\n",
    "    else:\n",
    "        entities = [t.get(\"entities\", []) for t in data.get(\"transcript\", [])]\n",
    "        entities = [e for sublist in entities for e in sublist]\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entities\"] = df[\"data\"].apply(get_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types = list(set([e[1] for sublist in df.entities for e in sublist]))\n",
    "entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"locations\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"LOC\" and len(e[0]) > 2])\n",
    "df[\"people\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"PER\" and len(e[0]) > 2])\n",
    "df[\"orgs\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"ORG\" and len(e[0]) > 1])\n",
    "df[\"misc\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"MISC\" and len(e[0]) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"media_id\", \"rts_id\", \"year\", \"data\", \"categoryName\", \"contentType\", \"title\", \"resume\", \"mediaDuration\", \"locations\", \"people\", \"orgs\", \"misc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/rts_sample.csv\", index=False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load presaved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/rts_sample.csv\", \n",
    "                 sep = \"\\t\", \n",
    "                 converters = {\n",
    "                     \"data\": literal_eval,\n",
    "                     \"locations\": literal_eval,\n",
    "                     \"people\": literal_eval,\n",
    "                     \"orgs\": literal_eval,\n",
    "                     \"misc\": literal_eval\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"transcript\"] = df[\"data\"].map(lambda x: x.get(\"transcript\", []))\n",
    "df = df[df.transcript.map(lambda x: type(x) == list)].reset_index(drop=True) # Get full videos with speaker diarization info\n",
    "print(f\"Processed {len(df)} videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "df.groupby(\"rts_id\").categoryName.agg(set).map(lambda x: list(x)[0]).value_counts().plot(kind=\"barh\")\n",
    "plt.title(\"Number of videos per category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "df.rts_id.value_counts().hist(bins=100, grid = False)\n",
    "plt.title(\"Number of clips extracted per video\")\n",
    "plt.text(100, 1200, f\"{len(df)} clips extracted\\nout of {len(df.rts_id.unique())} videos\", fontdict={\"size\": 12, \"weight\": \"bold\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "df.groupby(\"rts_id\").year.mean().hist(bins=50, ax=axs[0], grid = False)\n",
    "axs[0].set_title(\"Distribution of the year of publication of the videos\")\n",
    "df.year.hist(bins=50, ax=axs[1], grid = False)\n",
    "axs[1].set_title(\"Distribution of the year of publication of the clips\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map of Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[\"locations\"].explode().value_counts()\n",
    "locations = pd.DataFrame(locations).reset_index().rename(columns={\"locations\":\"location\"})\n",
    "locations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[[\"locations\", \"year\"]].explode(\"locations\").dropna().groupby(\"locations\").agg(list).reset_index()\n",
    "locations[\"count\"] = locations[\"year\"].apply(len)\n",
    "locations[\"year\"] = locations.year.map(lambda x: Counter(x))\n",
    "locations = locations.sort_values(\"count\", ascending=False)\n",
    "locations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"emv/features/cities.json\", \"r\") as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame([{\"locations\":k, \"lon\":float(v[0]), \"lat\":float(v[1])} for k,v in cities.items() if len(v) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities = pd.merge(locations, cities, on=\"locations\", how=\"left\").dropna(subset = [\"lat\", \"lon\"]).reset_index()\n",
    "found_cities[\"first_mention\"] = found_cities[\"year\"].map(lambda x: min(x.keys()))\n",
    "found_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branca.colormap as cm\n",
    "\n",
    "colormap = cm.linear.plasma.scale(found_cities[\"first_mention\"].min(), found_cities[\"first_mention\"].max()).to_step(10)\n",
    "\n",
    "# Create a base map centered around Switzerland\n",
    "m = folium.Map(location=[46.8182, 8.2275], zoom_start=8)\n",
    "size_multiplier = 2\n",
    "# Add city points to the map\n",
    "for index, row in found_cities.iterrows():\n",
    "    color = colormap(row['first_mention'])\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lon'], row['lat']),\n",
    "        radius = np.sqrt(row['count'] / np.pi) * size_multiplier,\n",
    "        color=\"black\",\n",
    "        weight = 1,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.6,\n",
    "        tooltip=row['locations'] + ': ' + str(row['count']) + ' occurrences - First mentioned in ' + str(row['first_mention']),\n",
    "    ).add_to(m)\n",
    "\n",
    "m.add_child(colormap)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mentions_per_year = found_cities.year.sum()\n",
    "n_mentions_per_year = pd.DataFrame(n_mentions_per_year.items(), columns=[\"year\", \"count\"])\n",
    "n_mentions_per_year = n_mentions_per_year.sort_values(\"year\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.lineplot(data=n_mentions_per_year, x=\"year\", y=\"count\")\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mentions_per_year_dict = n_mentions_per_year.set_index(\"year\").to_dict()[\"count\"]\n",
    "found_cities[\"relative_counts\"] = found_cities.year.map(lambda x: {k:v / n_mentions_per_year_dict[k] for k,v in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "found_cities[:20].sort_values(\"count\").set_index(\"locations\")[\"count\"].plot(kind=\"barh\")\n",
    "plt.xlabel(\"Number of mentions\")\n",
    "plt.title(\"Top 20 locations mentioned in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 20\n",
    "skip_first_n = 1\n",
    "top_cities = found_cities[skip_first_n:top_N + skip_first_n]\n",
    "counts_per_year = pd.DataFrame(top_cities.year.tolist(), index=top_cities.locations).fillna(0)\n",
    "counts_per_year = counts_per_year.T.sort_index().T\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(counts_per_year, cmap=\"Blues\", cbar_kws={'label': 'Number of mentions'})\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for loc,mentions in counts_per_year.iterrows():\n",
    "    plt.plot(mentions.index, mentions.values, label=loc)\n",
    "plt.legend()\n",
    "plt.title(f\"Number of mentions of the top {len(counts_per_year)} locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 20\n",
    "skip_first_n = 1\n",
    "top_cities = found_cities[skip_first_n:top_N + skip_first_n]\n",
    "counts_per_year = pd.DataFrame(top_cities.relative_counts.tolist(), index=top_cities.locations).fillna(0)\n",
    "counts_per_year = counts_per_year.T.sort_index().T\n",
    "\n",
    "counts_per_year = counts_per_year.div(counts_per_year.sum(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(counts_per_year, cmap=\"Blues\", cbar_kws={'label': 'Number of mentions'})\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for loc,mentions in counts_per_year.iterrows():\n",
    "    plt.plot(mentions.index, mentions.values, label=loc)\n",
    "plt.legend()\n",
    "plt.title(f\"Number of mentions of the top {len(counts_per_year)} locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_people = [\"messieurs\", \"monsieur\", \"madame\", \"mesdames\"]\n",
    "\n",
    "df[\"people\"] = df[\"people\"].apply(lambda x: [p for p in x if p.lower() not in filter_people])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = df[[\"people\", \"year\"]].explode(\"people\").dropna().groupby(\"people\").agg(list).reset_index()\n",
    "persons[\"count\"] = persons[\"year\"].apply(len)\n",
    "persons[\"year\"] = persons.year.map(lambda x: Counter(x))\n",
    "persons = persons.sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = persons.head(1000)\n",
    "top_persons.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"wikidata_search\"] = top_persons[\"people\"].apply(get_wikidata_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = top_persons[top_persons[\"wikidata_search\"].map(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"wikidata_id\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"id\", \"MISSING_ID\") if len(x) > 0 else None)\n",
    "top_persons[\"wikidata_label\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"label\", \"MISSING_LABEL\") if len(x) > 0 else None)\n",
    "top_persons[\"wikidata_description\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"description\", \"MISSING_DESCRIPTION\") if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_of = process_batch(top_persons[\"wikidata_id\"].dropna().tolist(), \"P31\", BATCH_SIZE=20)\n",
    "top_persons[\"instance_of\"] = top_persons[\"wikidata_id\"].map(instance_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = top_persons.instance_of.dropna().unique().tolist()\n",
    "instances = {i:get_wikidata_label(i.split(\"/\")[-1]) for i in instances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"instance_of\"] = top_persons[\"instance_of\"].map(instances)\n",
    "top_persons[\"instance_of\"] = top_persons[\"instance_of\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"instance_of\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on humans (instance of Q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons.dropna(subset=[\"wikidata_id\", \"instance_of\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = top_persons[top_persons.instance_of == \"human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = top_persons[\"wikidata_id\"].tolist()\n",
    "citizenship = process_batch(ids, \"P27\", BATCH_SIZE=20)\n",
    "top_persons[\"citizenship\"] = top_persons[\"wikidata_id\"].map(citizenship)\n",
    "\n",
    "occupation = process_batch(ids, \"P106\", BATCH_SIZE=20)\n",
    "top_persons[\"occupation\"] = top_persons[\"wikidata_id\"].map(occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizenship_labels = {k:get_wikidata_label(k.split(\"/\")[-1]) for k in top_persons[\"citizenship\"].dropna().unique()}\n",
    "top_persons[\"citizenship\"] = top_persons[\"citizenship\"].map(citizenship_labels)\n",
    "\n",
    "occupation_labels = {k:get_wikidata_label(k.split(\"/\")[-1]) for k in top_persons[\"occupation\"].dropna().unique()}\n",
    "top_persons[\"occupation\"] = top_persons[\"occupation\"].map(occupation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"citizenship\"] = top_persons[\"citizenship\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")\n",
    "top_persons[\"occupation\"] = top_persons[\"occupation\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "top_persons.occupation.value_counts()[:20].plot(kind=\"barh\", ax=axs[0])\n",
    "axs[0].set_title(\"Occupations of most occurring persons\")\n",
    "top_persons.citizenship.value_counts()[:20].plot(kind=\"barh\", ax=axs[1])\n",
    "axs[1].set_title(\"Citizenships of most occurring persons\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[[\"media_id\", \"rts_id\", \"year\", \"mediaDuration\", \"transcript\"]].explode(\"transcript\")\n",
    "sentences.dropna(subset=\"transcript\", inplace=True)\n",
    "sentences[\"transcript\"] = sentences.transcript.map(lambda x: x.get(\"t\", \"\")).tolist()\n",
    "print(f\"Extracted {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[\"sentence_length\"] = sentences.transcript.map(lambda x: len(x.split()))\n",
    "sentences.sentence_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sentences.year.value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.xticks(range(0, len(sentences.year.value_counts().index), 5), rotation=0)\n",
    "plt.ylabel(\"Number of sentences\")\n",
    "plt.title(\"Number of sentences per year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample N sentences per year\n",
    "N = 100\n",
    "sampled_sentences = pd.DataFrame()\n",
    "for year, group in sentences.groupby(\"year\"):\n",
    "    if N > len(group):\n",
    "        sampled_group = group\n",
    "        print(f\"Only sampled {len(group)} sentences for year {year}\")\n",
    "    else:\n",
    "        sampled_group = group.sample(N)\n",
    "    sampled_sentences = pd.concat([sampled_sentences, sampled_group])\n",
    "sampled_sentences.reset_index(drop=True, inplace=True)\n",
    "print(f\"\\nSampled {len(sampled_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences.transcript.map(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = pipeline(\n",
    "    model = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "    top_k = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"sentiment_scores\"] = sampled_sentences.transcript.map(lambda x: sentiment_classifier(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_score(positive_score, neutral_score, negative_score):\n",
    "    sentiment_score = positive_score - negative_score\n",
    "    return sentiment_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences.dropna(subset=[\"sentiment_scores\"], inplace=True)\n",
    "\n",
    "sampled_sentences[\"sentiment_scores\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: {s[\"label\"]:s[\"score\"] for s in x})\n",
    "sampled_sentences[\"positive_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: x.get(\"positive\", 0))\n",
    "sampled_sentences[\"negative_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: x.get(\"negative\", 0))\n",
    "sampled_sentences[\"neutral_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: x.get(\"neutral\", 0))\n",
    "sampled_sentences[\"top_sentiment\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: max(x, key=x.get))\n",
    "sampled_sentences[\"top_sentiment_score\"] = sampled_sentences[\"sentiment_scores\"].map(lambda x: max(x.values()))\n",
    "\n",
    "def calculate_sentiment_score(positive_score, neutral_score, negative_score):\n",
    "    sentiment_score = positive_score - negative_score\n",
    "    return sentiment_score\n",
    "\n",
    "sampled_sentences[\"sentiment_score\"] = sampled_sentences[[\"positive_score\", \"neutral_score\", \"negative_score\"]].apply(lambda x: calculate_sentiment_score(*x), axis=1)\n",
    "sampled_sentences.drop(columns = [\"sentiment_scores\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sampled_sentences.top_sentiment.value_counts().reindex([\"positive\", \"negative\", \"neutral\"]).plot(kind=\"bar\", ax=axs[0])\n",
    "axs[0].set_title(\"Sentiment distribution\")\n",
    "axs[0].set_xticks(range(3), [\"positive\", \"negative\", \"neutral\"], rotation=0)\n",
    "\n",
    "sampled_sentences.sentiment_score.hist(ax=axs[1], bins=100, grid = False)\n",
    "axs[1].set_title(\"Sentiment score distribution\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top k extreme sentences, by sentiment\n",
    "k = 50\n",
    "print(f\"Top {k} positive sentences\")\n",
    "_ = [print(s) for s in sampled_sentences[sampled_sentences.top_sentiment == \"positive\"].sort_values(\"top_sentiment_score\", ascending=False).transcript.tolist()[:k]]\n",
    "print()\n",
    "print(f\"Top {k} negative sentences\")\n",
    "_ = [print(s) for s in sampled_sentences[sampled_sentences.top_sentiment == \"negative\"].sort_values(\"top_sentiment_score\", ascending=False).transcript.tolist()[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = sampled_sentences.groupby(\"media_id\").sentiment_score.mean()\n",
    "clips = clips.reset_index()\n",
    "clips[\"sentiment_std\"] = sampled_sentences.groupby(\"media_id\").sentiment_score.std().fillna(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].hist(clips.sentiment_score, bins = 100)\n",
    "axs[0].set_title(\"Distribution of the mean sentiment score\")\n",
    "\n",
    "axs[1].hist(clips.sentiment_std, bins = 50)\n",
    "axs[1].set_title(\"Distribution of the standard deviation of the sentiment score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sentiment_scores_per_year = sampled_sentences.groupby(\"year\").sentiment_score.mean().reset_index()\n",
    "sentiment_scores_per_year[\"sentiment_std\"] = sampled_sentences.groupby(\"year\").sentiment_score.std().fillna(0).tolist() / np.sqrt(sampled_sentences.groupby(\"year\").sentiment_score.count().tolist())\n",
    "sentiment_scores_per_year.sort_values(\"year\", inplace=True)\n",
    "\n",
    "plt.errorbar(sentiment_scores_per_year.year, sentiment_scores_per_year.sentiment_score, yerr=sentiment_scores_per_year.sentiment_std, fmt='o')\n",
    "plt.hlines(0, 1949, 2022, color=\"black\", linestyle=\"--\")\n",
    "plt.xlim(1949, 2022)  # Limit the x-axis view\n",
    "plt.ylabel(\"Mean sentiment score\")\n",
    "plt.title(\"Mean sentiment score of sentences per year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[sampled_sentences.year == 1957][[\"transcript\", \"sentiment_score\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"emotions\"] = sampled_sentences.transcript.apply(lambda x: classifier(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"emotions\"] = sampled_sentences[\"emotions\"].map(lambda x: {s[\"label\"]:s[\"score\"] for s in x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"top_emotion\"] = sampled_sentences[\"emotions\"].map(lambda x: max(x, key=x.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_sentences[\"top_emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rts-bWoRmFur-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
