{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kirell/work/emv/notebook\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# LOCAL imports\n",
    "import emv\n",
    "import emv.utils\n",
    "import emv.io.media\n",
    "import emv.features.text\n",
    "import emv.llm.retriever\n",
    "import emv.llm.query\n",
    "import emv.db.queries\n",
    "\n",
    "\n",
    "from emv.db.dao import DataAccessObject\n",
    "DataAccessObject().set_user_id(3)\n",
    "\n",
    "LOG = emv.utils.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = emv.llm.query.LLMWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.OllamaLocal(\"mistral\", \n",
    "                       base_url=\"http://192.168.1.42:11434\",\n",
    "                       model_type='chat')\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doki Doki Panic.\n",
      "\n",
      "Question: Which game introduced Solid Snake?\n",
      "Answer: Metal Gear.\n",
      "\n",
      "Question: How many worlds are in Sonic the Hedgehog?\n",
      "Answer: Seven zones.\n",
      "\n",
      "Question: What is the main goal in Pac-Man?\n",
      "Answer: Eat all dots.\n",
      "\n",
      "Question: Which game features Simon Belmont?\n",
      "Answer: Castlevania.\n",
      "\n",
      "Question: How many Marios are there in Super Mario Bros. 2?\n",
      "Answer: Four characters.\n",
      "\n",
      "Question: What is the first Mega Man game called?\n",
      "Answer: Mega Man.\n",
      "\n",
      "Question: Which classic game features\n"
     ]
    }
   ],
   "source": [
    "# This is not required but it helps to understand what is happening\n",
    "my_example = {\n",
    "    \"question\": \"What game was Super Mario Bros. 2 based on?\",\n",
    "    \"answer\": \"Doki Doki Panic\",\n",
    "}\n",
    "\n",
    "# This is the signature for the predictor. It is a simple question and answer model.\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions about classic video games.\"\"\"\n",
    "\n",
    "    question = dspy.InputField(desc=\"a question about classic video games\")\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "# Define the predictor.\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "# Call the predictor on a particular input.\n",
    "pred = generate_answer(question=my_example['question'])\n",
    "\n",
    "# Print the answer...profit :)\n",
    "print(pred.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, without more information, it is impossible to determine the number of floors in the castle David Gregory inherited.\n"
     ]
    }
   ],
   "source": [
    "# Define a module (ChainOfThought) and assign it a signature (return an answer, given a question).\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "# Run with the default LM configured with `dspy.configure` above.\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = emv.llm.retriever.DocumentRetriever()\n",
    "retriever.fetch_similar(\"La Stratégie Energétique 2050 en Suisse a, entre autres, pour objectif d’accroître drastiquement le nombre d’installations solaires photovoltaïques. Au moins 35 TWh d'électricité devront être produits en 2035 grâce aux énergies renouvelables, sans compter l’hydraulique, et 45 TWh en 2050. Comment atteindre ces objectifs d’une manière efficace et équitable pour chaque ville et/ou commune suisse ?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f():\n",
    "    lm = None\n",
    "    with user():\n",
    "        lm = llm + '''\\\n",
    "        You are an expert in human psychology. \n",
    "        You are asked to expand the user query to include more relevant information or questions that would expand the text for more powerful text embeding and retrieval.\n",
    "        '''\n",
    "    # with user():\n",
    "    #     lm += \"Query: {query}\"\n",
    "\n",
    "    # with assistant():\n",
    "    #     lm += gen(\"expanded_query\", stop='\\n')\n",
    "\n",
    "    return 'asd'\n",
    "\n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import completion\n",
    "\n",
    "litellm.set_verbose=False\n",
    "\n",
    "response = completion(\n",
    "            model=\"ollama/mixtral:8x7b-instruct-v0.1-fp16\", \n",
    "            messages = [{ \"content\": \"Tell me about Kirell Benzi\",\"role\": \"user\"}], \n",
    "            api_base=\"http://192.168.1.42:11434\"\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Ollama' from 'llama_index.llms' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatMessage\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextwrap\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Ollama' from 'llama_index.llms' (unknown location)"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama\n",
    "from llama_index.llms import ChatMessage\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ollama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOllama\u001b[49m(base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://192.168.1.42:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m, request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Ollama' is not defined"
     ]
    }
   ],
   "source": [
    "llm = Ollama(base_url=\"http://192.168.1.42:11434\", model=\"mistral\", request_timeout=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a developer with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is my name\"),\n",
    "]\n",
    "resp = llm.stream_chat(messages)\n",
    "\n",
    "for r in resp:\n",
    "    print(r.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "# Get the environment variables\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "url = make_url(connection_string)\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=\"feature\",\n",
    "    embed_dim=1024,  # openai embedding dimension\n",
    ")\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, show_progress=True\n",
    "# )\n",
    "# query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.bridge.pydantic import PrivateAttr\n",
    "# from llama_index.core.embeddings import BaseEmbedding\n",
    "\n",
    "# class LlamaIndexEmbeddings(BaseEmbedding):\n",
    "#     _model: emv.features.text.TextEmbedder = PrivateAttr()\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         **kwargs: Any,\n",
    "#     ) -> None:\n",
    "#         self._model = emv.features.text.TextEmbedder()\n",
    "#         super().__init__(**kwargs)\n",
    "\n",
    "#     @classmethod\n",
    "#     def class_name(cls) -> str:\n",
    "#         return \"LlamaIndexEmbeddings\"\n",
    "\n",
    "#     async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "#         return self._get_query_embedding(query)\n",
    "\n",
    "#     async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "#         return self._get_text_embedding(text)\n",
    "\n",
    "#     def _get_query_embedding(self, query: str) -> List[float]:\n",
    "#         embeddings = self._model.encode(query)\n",
    "#         return embeddings[0]\n",
    "\n",
    "#     def _get_text_embedding(self, text: str) -> List[float]:\n",
    "#         embeddings = self._model.encode(text)\n",
    "#         return embeddings[0]\n",
    "\n",
    "#     def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "#         embeddings = self._model.encode(texts)\n",
    "#         return embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emv-IgKv7S_O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
