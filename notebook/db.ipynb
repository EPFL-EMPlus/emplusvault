{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Examples \n",
    "\n",
    "Examples for the new database schema and how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /home/andre/rts/.venv/lib/python3.10/site-packages (2.9.6)\n",
      "Requirement already satisfied: python-dotenv in /home/andre/rts/.venv/lib/python3.10/site-packages (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install psycopg2-binary python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andre/rts\n"
     ]
    }
   ],
   "source": [
    "# jupyter notebook auto reload\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Utility functions for setting up the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from rts.db.utils import execute_read_query, execute_write_query\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "Creating the necessary database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_write_query(\"DROP TABLE IF EXISTS map_projection_feature;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS projection;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS features;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS media;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS library;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,)]\n"
     ]
    }
   ],
   "source": [
    "# postgres table definitions\n",
    "\n",
    "_table_library = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS library (\n",
    "        library_id SERIAL PRIMARY KEY,\n",
    "        library_name VARCHAR(50) NOT NULL,\n",
    "        version VARCHAR(20) NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        data JSONB NOT NULL\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_create_library = \"\"\"\n",
    "    INSERT INTO library (library_name, version, data)\n",
    "    VALUES ('rts', '0.1', '{}')\n",
    "    RETURNING library_id;\n",
    "\"\"\"\n",
    "\n",
    "_table_projection = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS projection (\n",
    "        projection_id SERIAL PRIMARY KEY,\n",
    "        version VARCHAR(20) NOT NULL,\n",
    "        library_id INTEGER NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        model_name VARCHAR(200) NOT NULL,\n",
    "        model_params JSONB NOT NULL,\n",
    "        data JSONB NOT NULL,\n",
    "        dimension INTEGER NOT NULL,\n",
    "        atlas_folder_path VARCHAR(500) NOT NULL,\n",
    "        atlas_width INTEGER NOT NULL,\n",
    "        tile_size INTEGER NOT NULL,\n",
    "        atlas_count INTEGER NOT NULL,\n",
    "        total_tiles INTEGER NOT NULL,\n",
    "        tiles_per_atlas INTEGER NOT NULL,\n",
    "\n",
    "        CONSTRAINT FK_projection_library_id FOREIGN KEY (library_id)\n",
    "            REFERENCES library (library_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_table_media = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS media (\n",
    "        media_id SERIAL PRIMARY KEY,\n",
    "        media_path VARCHAR(500) UNIQUE,\n",
    "        original_path VARCHAR(500) NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        media_type VARCHAR(50) NOT NULL,\n",
    "        sub_type VARCHAR(50) NOT NULL,\n",
    "        size INTEGER NOT NULL,\n",
    "        metadata JSONB NOT NULL,\n",
    "        library_id INTEGER NOT NULL,\n",
    "        hash VARCHAR(50) UNIQUE,\n",
    "        parent_id INTEGER,\n",
    "        start_ts FLOAT,\n",
    "        end_ts FLOAT,\n",
    "        start_frame INTEGER,\n",
    "        end_frame INTEGER,\n",
    "        frame_rate FLOAT,\n",
    "\n",
    "        CONSTRAINT FK_media_library_id FOREIGN KEY (library_id)\n",
    "            REFERENCES library (library_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_table_features = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS features (\n",
    "        feature_id SERIAL PRIMARY KEY,\n",
    "        feature_type VARCHAR(50) NOT NULL,\n",
    "        version VARCHAR(20) NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        model_name VARCHAR(200) NOT NULL,\n",
    "        model_params JSONB NOT NULL,\n",
    "        data JSONB NOT NULL,\n",
    "\n",
    "        embedding_size INTEGER,\n",
    "        embedding_1024 vector (1024),\n",
    "        embedding_1536 vector (1536),\n",
    "        embedding_2048 vector (2048),\n",
    "\n",
    "        media_id INTEGER,\n",
    "\n",
    "        CONSTRAINT FK_features_media_id FOREIGN KEY (media_id) \n",
    "            REFERENCES media (media_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_table_map_projection_feature = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS map_projection_feature (\n",
    "        map_projection_feature_id SERIAL PRIMARY KEY,\n",
    "        projection_id INTEGER NOT NULL,\n",
    "        feature_id INTEGER,\n",
    "        media_id INTEGER,\n",
    "        atlas_order INTEGER NOT NULL,\n",
    "\n",
    "        CONSTRAINT FK_map_projection_feature_projection_id FOREIGN KEY (projection_id)\n",
    "            REFERENCES projection (projection_id),\n",
    "        CONSTRAINT FK_map_projection_feature_feature_id FOREIGN KEY (feature_id)\n",
    "            REFERENCES features (feature_id),\n",
    "        CONSTRAINT FK_map_projection_feature_media_id FOREIGN KEY (media_id)\n",
    "            REFERENCES media (media_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "execute_write_query(_table_library)\n",
    "execute_write_query(_table_projection)\n",
    "execute_write_query(_table_media)\n",
    "execute_write_query(_table_features)\n",
    "execute_write_query(_table_map_projection_feature)\n",
    "\n",
    "print(execute_read_query(_create_library))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill tables with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample data for the tables, each media element can have multiple features\n",
    "\n",
    "swiss_cities = [\"Zurich\", \"Geneva\", \"Basel\", \"Lausanne\", \"Bern\", \"Winterthur\", \"Lucerne\", \"St. Gallen\", \"Lugano\", \"Biel/Bienne\"]\n",
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    _table_media_sample = f\"\"\"\n",
    "        INSERT INTO media (media_path, original_path, media_type, sub_type, size, archive_name, archive_id, metadata)\n",
    "        VALUES \n",
    "            ('/path/to/media{i}', '/path/to/original{i}', 'image', 'jpg', 500, 'archive{i}', 'archive_id{i}', '{{\"key{i}\": \"value{i}\", \"city\": \"{swiss_cities[(i-1) % len(swiss_cities)]}\", \"year\": {years[(i-1) % len(years)]}}}')\n",
    "        RETURNING media_id;\n",
    "    \"\"\"\n",
    "    execute_write_query(_table_media_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '/path/to/media1',\n",
       "  '/path/to/original1',\n",
       "  datetime.datetime(2023, 5, 2, 10, 35, 53, 70041),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  'archive1',\n",
       "  'archive_id1',\n",
       "  {'city': 'Zurich', 'key1': 'value1', 'year': 2015}),\n",
       " (2,\n",
       "  '/path/to/media2',\n",
       "  '/path/to/original2',\n",
       "  datetime.datetime(2023, 5, 2, 10, 35, 53, 107340),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  'archive2',\n",
       "  'archive_id2',\n",
       "  {'city': 'Geneva', 'key2': 'value2', 'year': 2016})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"\"\"SELECT * FROM media LIMIT 2;\"\"\"\n",
    "execute_read_query(_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert some samples with vectors\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "feature_types = [\"pose\", \"face\", \"object\", \"ner\"]\n",
    "color_meta = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"white\", \"grey\", \"orange\", \"purple\", \"pink\"]\n",
    "\n",
    "for i in range(1, 11):\n",
    "    vector_1024 = np.random.rand(1024).tolist()\n",
    "    vector_2048 = np.random.rand(2048).tolist()\n",
    "    ner_tags = json.dumps([(\"person\", \"Ueli Steck\", 1, 9), (\"city\", \"geneva\", 10, 16), (\"city\", \"zurich\", 17, 23), (\"city\", \"bern\", 24, 28), (\"city\", \"basel\", 29, 34), (\"city\", \"winterthur\", 35, 46), (\"city\", \"lucerne\", 47, 54), (\"city\", \"st. gallen\", 55, 65), (\"city\", \"lugano\", 66, 72), (\"city\", \"biel/bienne\", 73, 85)])\n",
    "\n",
    "    _table_features_sample_vectors = f\"\"\"\n",
    "        INSERT INTO features (feature_type, version, model_name, model_params, data, media_id, embedding_size, embedding_1024)\n",
    "        VALUES\n",
    "            ('{feature_types[i % len(feature_types)]}', 'v1', 'resnet50', '{{\"param1\": \"{i}\"}}', '{{\"color\": \"{color_meta[i % len(color_meta)] }\", \"data1\": \"{i}\", \"ner\": {ner_tags} }}', {i}, 1024, ARRAY[{','.join([str(x) for x in vector_1024])}])\n",
    "        RETURNING feature_id;\n",
    "    \"\"\"\n",
    "    # print(_table_features_sample_vectors)\n",
    "    execute_write_query(_table_features_sample_vectors)\n",
    "\n",
    "    # At the moment we are only creating size 1024 vectors, for the sake of the next example queries to work, there can be only a single vector set per feature\n",
    "    # _table_features_sample_vectors = f\"\"\"\n",
    "    #     INSERT INTO features (feature_type, version, model_name, model_params, data, media_id, embedding_size, embedding_2048)\n",
    "    #     VALUES\n",
    "    #         ('image', 'v1', 'resnet50', '{{\"param1\": \"{i}\"}}', '{{\"data1\": \"{i}\"}}', {i}, 2048, ARRAY[{','.join([str(x) for x in vector_2048])}])\n",
    "    #     RETURNING feature_id;\n",
    "    # \"\"\"\n",
    "\n",
    "    # execute_write_query(_table_features_sample_vectors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "### Scenario 1\n",
    "\n",
    "We have a media object and we want to find the 5 most similar media objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 12.6620561267192),\n",
       " (1, 12.9727147189136),\n",
       " (7, 12.9900472952322),\n",
       " (8, 13.0338304337032),\n",
       " (2, 13.0584285629705)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"\"\"\n",
    "    WITH target_embedding AS (\n",
    "    SELECT\n",
    "        media_id,\n",
    "        CASE\n",
    "            WHEN embedding_size = 1024 THEN embedding_1024\n",
    "            WHEN embedding_size = 2048 THEN embedding_2048\n",
    "            ELSE NULL\n",
    "        END AS embedding_vector\n",
    "    FROM \n",
    "        features\n",
    "    WHERE \n",
    "        media_id = 5\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "    f.media_id,\n",
    "    (target.embedding_vector <-> \n",
    "        CASE\n",
    "        WHEN f.embedding_size = 1024 THEN f.embedding_1024\n",
    "        WHEN f.embedding_size = 2048 THEN f.embedding_2048\n",
    "        ELSE NULL\n",
    "        END\n",
    "    ) AS distance\n",
    "    FROM\n",
    "    features f,\n",
    "    target_embedding target\n",
    "    WHERE\n",
    "    f.media_id != target.media_id\n",
    "    ORDER BY\n",
    "    distance ASC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "\n",
    "Find all media objects for Zurich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '/path/to/media1',\n",
       "  '/path/to/original1',\n",
       "  datetime.datetime(2023, 5, 2, 10, 35, 53, 70041),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  'archive1',\n",
       "  'archive_id1',\n",
       "  {'city': 'Zurich', 'key1': 'value1', 'year': 2015}),\n",
       " (11,\n",
       "  '/path/to/media11',\n",
       "  '/path/to/original11',\n",
       "  datetime.datetime(2023, 5, 2, 10, 35, 53, 446582),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  'archive11',\n",
       "  'archive_id11',\n",
       "  {'city': 'Zurich', 'year': 2019, 'key11': 'value11'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all media objects that have city: \"Zurich\" (queried from the jsonb metadata field)\n",
    "_query = \"\"\"\n",
    "    SELECT * FROM media WHERE metadata->>'city' = 'Zurich';\n",
    "\"\"\"\n",
    "execute_read_query(_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3\n",
    "\n",
    "Get all images from Geneva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '/path/to/media2',\n",
       "  '/path/to/original2',\n",
       "  datetime.datetime(2023, 5, 2, 10, 35, 53, 107340),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  'archive2',\n",
       "  'archive_id2',\n",
       "  {'city': 'Geneva', 'key2': 'value2', 'year': 2016}),\n",
       " (12,\n",
       "  '/path/to/media12',\n",
       "  '/path/to/original12',\n",
       "  datetime.datetime(2023, 5, 2, 10, 35, 53, 479945),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  'archive12',\n",
       "  'archive_id12',\n",
       "  {'city': 'Geneva', 'year': 2020, 'key12': 'value12'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"\"\"\n",
    "    SELECT * FROM media WHERE metadata->>'city' = 'Geneva' AND media_type = 'image';\n",
    "\"\"\"\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 4\n",
    "\n",
    "Fulltext string matching on jsonb fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Basel', 'Bern', 'Biel/Bienne', 'Basel', 'Bern', 'Biel/Bienne']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['St. Gallen', 'St. Gallen']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query for cities that start with b\n",
    "_query = \"\"\"\n",
    "    SELECT * FROM media WHERE metadata->>'city' LIKE 'B%';\n",
    "\"\"\"\n",
    "print([x[9]['city'] for x in execute_read_query(_query)])\n",
    "\n",
    "_query = \"\"\"\n",
    "    SELECT * FROM media WHERE metadata->>'city' LIKE '%Gall%';\n",
    "\"\"\"\n",
    "[x[9]['city'] for x in execute_read_query(_query)]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 5\n",
    "\n",
    "Similarity to a computed vector. Example: we have a video camera installed and a user poses like a tennis player and we find tennis matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 12.9973420438942), (4, 13.3367706705129)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_1024 = \",\".join([str(x) for x in np.random.rand(1024).tolist()])  # feature that would be creating by the pose detection algorithm\n",
    "\n",
    "# query the feature table for similar vectors\n",
    "_query = f\"\"\"\n",
    "    SELECT\n",
    "    f.media_id,\n",
    "    ('[{vector_1024}]' <-> \n",
    "        CASE\n",
    "        WHEN f.embedding_size = 1024 THEN f.embedding_1024\n",
    "        WHEN f.embedding_size = 2048 THEN f.embedding_2048\n",
    "        ELSE NULL\n",
    "        END\n",
    "    ) AS distance\n",
    "    FROM\n",
    "    features f\n",
    "    WHERE\n",
    "    f.feature_type = 'pose'\n",
    "    ORDER BY\n",
    "    distance ASC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 6\n",
    "\n",
    "Find people or locations with ner tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'green',\n",
       "   'data1': '1'}),\n",
       " (2,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'blue',\n",
       "   'data1': '2'}),\n",
       " (3,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'yellow',\n",
       "   'data1': '3'}),\n",
       " (4,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'black',\n",
       "   'data1': '4'}),\n",
       " (5,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'white',\n",
       "   'data1': '5'}),\n",
       " (6,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'grey',\n",
       "   'data1': '6'}),\n",
       " (7,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'orange',\n",
       "   'data1': '7'}),\n",
       " (8,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'purple',\n",
       "   'data1': '8'}),\n",
       " (9,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'pink',\n",
       "   'data1': '9'}),\n",
       " (10,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'red',\n",
       "   'data1': '10'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person = \"Ueli Steck\"\n",
    "\n",
    "_query = f\"\"\"\n",
    "    SELECT media_id, data FROM features WHERE data->>'ner' LIKE '%{person}%';\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 7\n",
    "\n",
    "Find aribitrary features by metadata (here we use the simple field color as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'red',\n",
       "   'data1': '10'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color = \"red\"\n",
    "\n",
    "_query = f\"\"\"\n",
    "    SELECT media_id, data FROM features WHERE data->>'color' = '{color}';\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest the clips to the media table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOTE_RTS_DATA = \"/media/sinergia/RTS/\"\n",
    "REMOTE_VIDEOS = '/mnt/rts/'\n",
    "\n",
    "LOCAL_RTS_DATA = \"/media/data/rts/\"\n",
    "METADATA = LOCAL_RTS_DATA + 'metadata'\n",
    "LOCAL_VIDEOS = LOCAL_RTS_DATA + 'archive'\n",
    "\n",
    "AIBOX = LOCAL_RTS_DATA + 'aibox-vectors'\n",
    "\n",
    "OUTDIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import hashlib\n",
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/rts/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# LOCAL imports\n",
    "import rts\n",
    "import rts.pipeline\n",
    "import rts.utils\n",
    "import rts.io.media\n",
    "import rts.features.audio\n",
    "import rts.features.text\n",
    "\n",
    "LOG = rts.utils.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_HOST\"), \n",
    "    os.getenv(\"SUPABASE_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"rts\"\n",
    "# res = supabase.storage.create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = rts.utils.dataframe_from_hdf5(LOCAL_RTS_DATA + '/metadata', 'rts_aivectors')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mediaId': 'ZE004015',\n",
       " 'media_folder': '/media/data/rts/archive/5/1/0/ZE004015',\n",
       " 'clip_folder': '/media/data/rts/archive/5/1/0/ZE004015/clips',\n",
       " 'image_resolutions': ['original', '32px', '64px', '128px', '256px', '512px'],\n",
       " 'framerate': 25.0,\n",
       " 'clip_count': 7,\n",
       " 'clips': {'ZE004015-L000': {'clip_id': 'ZE004015-L000',\n",
       "   'clip_number': 0,\n",
       "   'start': 28.0,\n",
       "   'start_frame': 700,\n",
       "   'end': 34.0,\n",
       "   'end_frame': 850,\n",
       "   'duration': 6.0,\n",
       "   'duration_frames': 150,\n",
       "   'images': ['ZE004015-L000-00.jpg',\n",
       "    'ZE004015-L000-01.jpg',\n",
       "    'ZE004015-L000-02.jpg'],\n",
       "   'ref_text': 'On direct du MAD de Lausanne et sur TSR2, une émission qui vous est proposée par la TSR',\n",
       "   'locations': 'Lausanne'},\n",
       "  'ZE004015-L001': {'clip_id': 'ZE004015-L001',\n",
       "   'clip_number': 1,\n",
       "   'start': 247.0,\n",
       "   'start_frame': 6175,\n",
       "   'end': 258.0,\n",
       "   'end_frame': 6450,\n",
       "   'duration': 11.0,\n",
       "   'duration_frames': 275,\n",
       "   'images': ['ZE004015-L001-00.jpg',\n",
       "    'ZE004015-L001-01.jpg',\n",
       "    'ZE004015-L001-02.jpg'],\n",
       "   'ref_text': 'Et derrière cette onomatopée se cache un quatuor de talent dont deux musiciennes classiques le tout, made in Fribourg, allions en rock, jazz, classique et pop, ça donne dans le mélange des genres sans pour autant se perdre...',\n",
       "   'locations': 'Fribourg'},\n",
       "  'ZE004015-L002': {'clip_id': 'ZE004015-L002',\n",
       "   'clip_number': 2,\n",
       "   'start': 2452.64,\n",
       "   'start_frame': 61316,\n",
       "   'end': 2458.64,\n",
       "   'end_frame': 61466,\n",
       "   'duration': 6.0,\n",
       "   'duration_frames': 150,\n",
       "   'images': ['ZE004015-L002-00.jpg',\n",
       "    'ZE004015-L002-01.jpg',\n",
       "    'ZE004015-L002-02.jpg'],\n",
       "   'ref_text': 'mettront vos seins en émoi aux bikinétesses de la Chaux-de-Fonds.',\n",
       "   'locations': 'La Chaux-de-Fonds'},\n",
       "  'ZE004015-L003': {'clip_id': 'ZE004015-L003',\n",
       "   'clip_number': 3,\n",
       "   'start': 2458.64,\n",
       "   'start_frame': 61466,\n",
       "   'end': 2465.68,\n",
       "   'end_frame': 61642,\n",
       "   'duration': 7.04,\n",
       "   'duration_frames': 176,\n",
       "   'images': ['ZE004015-L003-00.jpg',\n",
       "    'ZE004015-L003-01.jpg',\n",
       "    'ZE004015-L003-02.jpg'],\n",
       "   'ref_text': 'Ce soir également, le roman dit de Lausanne sera salement électro ou ne sera pas,',\n",
       "   'locations': 'Lausanne'},\n",
       "  'ZE004015-L004': {'clip_id': 'ZE004015-L004',\n",
       "   'clip_number': 4,\n",
       "   'start': 2490.64,\n",
       "   'start_frame': 62266,\n",
       "   'end': 2497.68,\n",
       "   'end_frame': 62442,\n",
       "   'duration': 7.04,\n",
       "   'duration_frames': 176,\n",
       "   'images': ['ZE004015-L004-00.jpg',\n",
       "    'ZE004015-L004-01.jpg',\n",
       "    'ZE004015-L004-02.jpg'],\n",
       "   'ref_text': \"Mercredi, Pigeon John recoulera comme un bienheureux à l'usine PTR de Genève.\",\n",
       "   'locations': 'Genève'},\n",
       "  'ZE004015-L005': {'clip_id': 'ZE004015-L005',\n",
       "   'clip_number': 5,\n",
       "   'start': 2501.68,\n",
       "   'start_frame': 62542,\n",
       "   'end': 2508.64,\n",
       "   'end_frame': 62716,\n",
       "   'duration': 6.96,\n",
       "   'duration_frames': 174,\n",
       "   'images': ['ZE004015-L005-00.jpg',\n",
       "    'ZE004015-L005-01.jpg',\n",
       "    'ZE004015-L005-02.jpg'],\n",
       "   'ref_text': 'Vendredi, vous aurez le choix du roi entre Belarus ou MAD de Genève',\n",
       "   'locations': 'Genève'},\n",
       "  'ZE004015-L006': {'clip_id': 'ZE004015-L006',\n",
       "   'clip_number': 6,\n",
       "   'start': 3633.28,\n",
       "   'start_frame': 90832,\n",
       "   'end': 3639.28,\n",
       "   'end_frame': 90982,\n",
       "   'duration': 6.0,\n",
       "   'duration_frames': 150,\n",
       "   'images': ['ZE004015-L006-00.jpg',\n",
       "    'ZE004015-L006-01.jpg',\n",
       "    'ZE004015-L006-02.jpg'],\n",
       "   'ref_text': \"Genève, capitale du rock'n'roll?\",\n",
       "   'locations': 'Genève'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load metadata\n",
    "# TODO: integrate into code example below\n",
    "\n",
    "clip_metadata = \"/media/data/rts/archive/5/1/0/ZE004015/clips.json\" \n",
    "with open(clip_metadata) as f:\n",
    "    clip_metadata = orjson.loads(f.read())\n",
    "    \n",
    "clip_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clip_id': 'ZE004015-L000',\n",
       " 'clip_number': 0,\n",
       " 'start': 28.0,\n",
       " 'start_frame': 700,\n",
       " 'end': 34.0,\n",
       " 'end_frame': 850,\n",
       " 'duration': 6.0,\n",
       " 'duration_frames': 150,\n",
       " 'images': ['ZE004015-L000-00.jpg',\n",
       "  'ZE004015-L000-01.jpg',\n",
       "  'ZE004015-L000-02.jpg'],\n",
       " 'ref_text': 'On direct du MAD de Lausanne et sur TSR2, une émission qui vous est proposée par la TSR',\n",
       " 'locations': 'Lausanne'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_metadata['clips']['ZE004015-L000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the clips from the archive and put them into supabase s3. At the same time create the database entries on the media table\n",
    "\n",
    "import json\n",
    "\n",
    "def write_media_object_db(\n",
    "        media_path: str, \n",
    "        original_path: str, \n",
    "        library_id: int, \n",
    "        parent_id: Optional[int] = None, \n",
    "        start_ts: Optional[int] = -1, \n",
    "        end_ts: Optional[int] = -1, \n",
    "        start_frame: Optional[int] = -1, \n",
    "        end_frame: Optional[int] = -1, \n",
    "        frame_rate: Optional[int] = -1, \n",
    "        update_data: Optional[Dict] = {},\n",
    "        file_size: Optional[int] = -1,\n",
    "        hash: Optional[str] = \"\",\n",
    "        media_type: str = 'video', \n",
    "        media_sub_type: str = \"clip\") -> str:\n",
    "    _query = f\"\"\"\n",
    "        INSERT INTO media (\n",
    "            media_path, original_path, media_type, sub_type, \n",
    "            size, library_id, metadata, hash, \n",
    "            parent_id, start_ts, end_ts, start_frame, \n",
    "            end_frame, frame_rate)\n",
    "        VALUES ('{media_path}', '{original_path}', '{media_type}', '{media_sub_type}', \n",
    "            {file_size}, {library_id}, \n",
    "            '{update_data}', '{hash}', \n",
    "            {parent_id}, {start_ts}, {end_ts}, {start_frame}, \n",
    "            {end_frame}, {frame_rate})\n",
    "        ON CONFLICT (media_id) DO NOTHING;\n",
    "    \"\"\"\n",
    "    execute_write_query(_query)\n",
    "\n",
    "\n",
    "def ingest_clips(df: pd.DataFrame, prefix_name: str, supabase: Client, bucket_name: str, library_id: int) -> Tuple[int, int]:\n",
    "    error_count = 0\n",
    "    no_clips = 0\n",
    "    default_format = \"mp4\"\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        base_path = os.path.join(row.mediaFolderPath.replace(REMOTE_VIDEOS, LOCAL_VIDEOS + '/'))\n",
    "        path = os.path.join(base_path, 'clips', 'videos')\n",
    "\n",
    "        # get all files in the folder\n",
    "        try:\n",
    "            files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "            no_clips += len(files)\n",
    "\n",
    "            with open(os.path.join(base_path, \"clips.json\")) as f:\n",
    "                clip_metadata = orjson.loads(f.read())\n",
    "\n",
    "            metadata = {\n",
    "                \"image_resolutions\": clip_metadata['image_resolutions'],\n",
    "                \"framerate\": clip_metadata['framerate'],\n",
    "                \"clip_count\": clip_metadata['clip_count'],\n",
    "            }\n",
    "\n",
    "            # TODO: Create parent media file for the source video for clips\n",
    "            parent_id = 1  # TODO: get the parent id from the database\n",
    "            original_path = f\"{base_path}.{default_format}\"\n",
    "            hash = hashlib.md5((original_path + str(0) + str(0)).encode('utf-8')).hexdigest()\n",
    "\n",
    "            write_media_object_db(\n",
    "                media_path=f\"{prefix_name}/{row.mediaFolderPath.replace(REMOTE_VIDEOS, '')}.{default_format}\",\n",
    "                original_path=original_path,\n",
    "                library_id=library_id,\n",
    "                parent_id=0,\n",
    "                update_data=json.dumps(metadata),\n",
    "                media_type='video',\n",
    "                media_sub_type='source_video',\n",
    "                frame_rate=clip_metadata['framerate'],\n",
    "                hash=hash,\n",
    "            )\n",
    "\n",
    "            # upload all files to supabase and create the database entries\n",
    "            for f in files:\n",
    "                supabase_key = f\"{prefix_name}/{row.mediaFolderPath.replace(REMOTE_VIDEOS, '')}/clips/videos/{f}\"\n",
    "                file_path = os.path.join(path, files[0])\n",
    "\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                \n",
    "                clip_name = f.split('.')[0]\n",
    "                start_ts = clip_metadata['clips'][clip_name]['start']\n",
    "                end_ts = clip_metadata['clips'][clip_name]['end']\n",
    "                start_frame = clip_metadata['clips'][clip_name]['start_frame']\n",
    "                end_frame = clip_metadata['clips'][clip_name]['end_frame']\n",
    "                frame_rate = clip_metadata['framerate']\n",
    "\n",
    "                hash = hashlib.md5((file_path + str(start_frame) + str(end_frame)).encode('utf-8')).hexdigest()\n",
    "\n",
    "                update_data = json.dumps(metadata | {'ref_text': clip_metadata['clips'][clip_name]['ref_text'], 'locations': clip_metadata['clips'][clip_name]['locations']})\n",
    "                update_data = update_data.replace(\"'\", \"''\")\n",
    "\n",
    "                write_media_object_db(\n",
    "                    media_path=supabase_key,\n",
    "                    original_path=os.path.abspath(file_path),\n",
    "                    library_id=library_id,\n",
    "                    parent_id=parent_id,\n",
    "                    start_ts=start_ts,\n",
    "                    end_ts=end_ts,\n",
    "                    start_frame=start_frame,\n",
    "                    end_frame=end_frame,\n",
    "                    frame_rate=frame_rate,\n",
    "                    update_data=update_data,\n",
    "                    file_size=file_size,\n",
    "                    hash=hash,\n",
    "                    media_type='video',\n",
    "                    media_sub_type='clip'\n",
    "                )\n",
    "\n",
    "                # upload to supabase s3\n",
    "                # supabase.storage.from_(bucket_name).upload(supabase_key, os.path.abspath(file_path))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"No clips folund for {error_count} rows\")\n",
    "    print(f\"Total number of clips: {no_clips}\")\n",
    "    print(f\"Total number of vidoes with clips: {len(df) - error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueViolation",
     "evalue": "duplicate key value violates unique constraint \"media_hash_key\"\nDETAIL:  Key (hash)=() already exists.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUniqueViolation\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m bucket_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrts\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m library_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m r \u001b[39m=\u001b[39m ingest_clips(df, \u001b[39m\"\u001b[39;49m\u001b[39mtest4\u001b[39;49m\u001b[39m\"\u001b[39;49m, supabase, bucket_name, library_id)\n",
      "Cell \u001b[0;32mIn[124], line 64\u001b[0m, in \u001b[0;36mingest_clips\u001b[0;34m(df, prefix_name, supabase, bucket_name, library_id)\u001b[0m\n\u001b[1;32m     61\u001b[0m original_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbase_path\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mdefault_format\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m \u001b[39mhash\u001b[39m \u001b[39m=\u001b[39m hashlib\u001b[39m.\u001b[39mmd5((original_path \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\u001b[39m.\u001b[39mhexdigest()\n\u001b[0;32m---> 64\u001b[0m write_media_object_db(\n\u001b[1;32m     65\u001b[0m     media_path\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mprefix_name\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mrow\u001b[39m.\u001b[39;49mmediaFolderPath\u001b[39m.\u001b[39;49mreplace(REMOTE_VIDEOS,\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m{\u001b[39;49;00mdefault_format\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     66\u001b[0m     original_path\u001b[39m=\u001b[39;49moriginal_path,\n\u001b[1;32m     67\u001b[0m     library_id\u001b[39m=\u001b[39;49mlibrary_id,\n\u001b[1;32m     68\u001b[0m     parent_id\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     69\u001b[0m     update_data\u001b[39m=\u001b[39;49mjson\u001b[39m.\u001b[39;49mdumps(metadata),\n\u001b[1;32m     70\u001b[0m     media_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvideo\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     71\u001b[0m     media_sub_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msource_video\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     72\u001b[0m     frame_rate\u001b[39m=\u001b[39;49mclip_metadata[\u001b[39m'\u001b[39;49m\u001b[39mframerate\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[39m# upload all files to supabase and create the database entries\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m files:\n",
      "Cell \u001b[0;32mIn[124], line 33\u001b[0m, in \u001b[0;36mwrite_media_object_db\u001b[0;34m(media_path, original_path, library_id, parent_id, start_ts, end_ts, start_frame, end_frame, frame_rate, update_data, file_size, hash, media_type, media_sub_type)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_media_object_db\u001b[39m(\n\u001b[1;32m      6\u001b[0m         media_path: \u001b[39mstr\u001b[39m, \n\u001b[1;32m      7\u001b[0m         original_path: \u001b[39mstr\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         media_type: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     19\u001b[0m         media_sub_type: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     20\u001b[0m     _query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39m        INSERT INTO media (\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[39m            media_path, original_path, media_type, sub_type, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m        ON CONFLICT (media_id) DO NOTHING;\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m     execute_write_query(_query)\n",
      "File \u001b[0;32m~/rts/rts/db/utils.py:29\u001b[0m, in \u001b[0;36mexecute_write_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m conn:\n\u001b[1;32m     28\u001b[0m     \u001b[39mwith\u001b[39;00m conn\u001b[39m.\u001b[39mcursor() \u001b[39mas\u001b[39;00m cur:\n\u001b[0;32m---> 29\u001b[0m         cur\u001b[39m.\u001b[39;49mexecute(query)\n\u001b[1;32m     30\u001b[0m     conn\u001b[39m.\u001b[39mcommit()\n",
      "\u001b[0;31mUniqueViolation\u001b[0m: duplicate key value violates unique constraint \"media_hash_key\"\nDETAIL:  Key (hash)=() already exists.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"rts\"\n",
    "library_id = \"1\"\n",
    "\n",
    "r = ingest_clips(df, \"test4\", supabase, bucket_name, library_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/rts/'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REMOTE_VIDEOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
