{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Examples \n",
    "\n",
    "Examples for the new database schema and how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /home/andre/rts/.venv/lib/python3.10/site-packages (2.9.6)\n",
      "Requirement already satisfied: python-dotenv in /home/andre/rts/.venv/lib/python3.10/site-packages (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install psycopg2-binary python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andre/rts\n"
     ]
    }
   ],
   "source": [
    "# jupyter notebook auto reload\n",
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Utility functions for setting up the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from emv.db.utils import execute_read_query, execute_write_query, write_media_object_db\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "Creating the necessary database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_write_query(\"DROP TABLE IF EXISTS map_projection_feature;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS projection;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS atlas;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS feature;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS media;\")\n",
    "execute_write_query(\"DROP TABLE IF EXISTS library;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,)]\n"
     ]
    }
   ],
   "source": [
    "# postgres table definitions\n",
    "\n",
    "_table_library = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS library (\n",
    "        library_id SERIAL PRIMARY KEY,\n",
    "        library_name VARCHAR(50) NOT NULL,\n",
    "        version VARCHAR(20) NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        data JSONB NOT NULL\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_create_library = \"\"\"\n",
    "    INSERT INTO library (library_name, version, data)\n",
    "    VALUES ('rts', '0.1', '{}')\n",
    "    RETURNING library_id;\n",
    "\"\"\"\n",
    "\n",
    "_table_projection = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS projection (\n",
    "        projection_id SERIAL PRIMARY KEY,\n",
    "        version VARCHAR(20) NOT NULL,\n",
    "        library_id INTEGER NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        model_name VARCHAR(200) NOT NULL,\n",
    "        model_params JSONB NOT NULL,\n",
    "        data JSONB NOT NULL,\n",
    "        dimension INTEGER NOT NULL,\n",
    "        atlas_folder_path VARCHAR(500) NOT NULL,\n",
    "        atlas_width INTEGER NOT NULL,\n",
    "        tile_size INTEGER NOT NULL,\n",
    "        atlas_count INTEGER NOT NULL,\n",
    "        total_tiles INTEGER NOT NULL,\n",
    "        tiles_per_atlas INTEGER NOT NULL,\n",
    "\n",
    "        CONSTRAINT FK_projection_library_id FOREIGN KEY (library_id)\n",
    "            REFERENCES library (library_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_table_atlas = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS atlas (\n",
    "        atlas_id SERIAL PRIMARY KEY,\n",
    "        projection_id INTEGER NOT NULL,\n",
    "        atlas_order INTEGER NOT NULL,\n",
    "        atlas_path VARCHAR(500) NOT NULL,\n",
    "        atlas_size Vector (2) NOT NULL,\n",
    "        tile_size Vector (2) NOT NULL,\n",
    "        tile_count INTEGER NOT NULL,\n",
    "        rows INTEGER NOT NULL,\n",
    "        cols INTEGER NOT NULL,\n",
    "        tiles_per_atlas INTEGER NOT NULL,\n",
    "\n",
    "        CONSTRAINT FK_atlas_projection_id FOREIGN KEY (projection_id)\n",
    "            REFERENCES projection (projection_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_table_media = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS media (\n",
    "        media_id SERIAL PRIMARY KEY,\n",
    "        media_path VARCHAR(500) UNIQUE,\n",
    "        original_path VARCHAR(500) NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        media_type VARCHAR(50) NOT NULL,\n",
    "        sub_type VARCHAR(50) NOT NULL,\n",
    "        size INTEGER NOT NULL,\n",
    "        metadata JSONB NOT NULL,\n",
    "        library_id INTEGER NOT NULL,\n",
    "        hash VARCHAR(50) UNIQUE,\n",
    "        parent_id INTEGER,\n",
    "        start_ts FLOAT,\n",
    "        end_ts FLOAT,\n",
    "        start_frame INTEGER,\n",
    "        end_frame INTEGER,\n",
    "        frame_rate FLOAT,\n",
    "\n",
    "        CONSTRAINT FK_media_library_id FOREIGN KEY (library_id)\n",
    "            REFERENCES library (library_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_table_features = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS feature (\n",
    "        feature_id SERIAL PRIMARY KEY,\n",
    "        feature_type VARCHAR(50) NOT NULL,\n",
    "        version VARCHAR(20) NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT NOW(),\n",
    "        model_name VARCHAR(200) NOT NULL,\n",
    "        model_params JSONB NOT NULL,\n",
    "        data JSONB NOT NULL,\n",
    "\n",
    "        embedding_size INTEGER,\n",
    "        embedding_1024 vector (1024),\n",
    "        embedding_1536 vector (1536),\n",
    "        embedding_2048 vector (2048),\n",
    "\n",
    "        media_id INTEGER,\n",
    "\n",
    "        CONSTRAINT FK_feature_media_id FOREIGN KEY (media_id) \n",
    "            REFERENCES media (media_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "_table_map_projection_feature = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS map_projection_feature (\n",
    "        map_projection_feature_id SERIAL PRIMARY KEY,\n",
    "        projection_id INTEGER NOT NULL,\n",
    "        feature_id INTEGER,\n",
    "        media_id INTEGER,\n",
    "        atlas_order INTEGER NOT NULL,\n",
    "\n",
    "        CONSTRAINT FK_map_projection_feature_projection_id FOREIGN KEY (projection_id)\n",
    "            REFERENCES projection (projection_id),\n",
    "        CONSTRAINT FK_map_projection_feature_feature_id FOREIGN KEY (feature_id)\n",
    "            REFERENCES feature (feature_id),\n",
    "        CONSTRAINT FK_map_projection_feature_media_id FOREIGN KEY (media_id)\n",
    "            REFERENCES media (media_id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "execute_write_query(_table_library)\n",
    "execute_write_query(_table_projection)\n",
    "execute_write_query(_table_media)\n",
    "execute_write_query(_table_features)\n",
    "execute_write_query(_table_map_projection_feature)\n",
    "\n",
    "print(execute_read_query(_create_library))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill tables with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample data for the tables, each media element can have multiple feature\n",
    "\n",
    "swiss_cities = [\"Zurich\", \"Geneva\", \"Basel\", \"Lausanne\", \"Bern\", \"Winterthur\", \"Lucerne\", \"St. Gallen\", \"Lugano\", \"Biel/Bienne\"]\n",
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "for i in range(1, 21):\n",
    "    _table_media_sample = f\"\"\"\n",
    "        INSERT INTO media (media_path, original_path, media_type, sub_type, size, metadata, library_id)\n",
    "        VALUES \n",
    "            ('/path/to/media{i}', '/path/to/original{i}', 'image', 'jpg', 500, '{{\"key{i}\": \"value{i}\", \"city\": \"{swiss_cities[(i-1) % len(swiss_cities)]}\", \"year\": {years[(i-1) % len(years)]}}}', 1)\n",
    "        RETURNING media_id;\n",
    "    \"\"\"\n",
    "    execute_write_query(_table_media_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,),\n",
       " (2,),\n",
       " (3,),\n",
       " (4,),\n",
       " (5,),\n",
       " (6,),\n",
       " (7,),\n",
       " (8,),\n",
       " (9,),\n",
       " (10,),\n",
       " (11,),\n",
       " (12,),\n",
       " (13,),\n",
       " (14,),\n",
       " (15,),\n",
       " (16,),\n",
       " (17,),\n",
       " (18,),\n",
       " (19,),\n",
       " (20,)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"\"\"SELECT media_id FROM media;\"\"\"\n",
    "execute_read_query(_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert some samples with vectors\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "feature_types = [\"pose\", \"face\", \"object\", \"ner\"]\n",
    "color_meta = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"white\", \"grey\", \"orange\", \"purple\", \"pink\"]\n",
    "\n",
    "for i in range(1, 11):\n",
    "    vector_1024 = np.random.rand(1024).tolist()\n",
    "    vector_2048 = np.random.rand(2048).tolist()\n",
    "    ner_tags = json.dumps([(\"person\", \"Ueli Steck\", 1, 9), (\"city\", \"geneva\", 10, 16), (\"city\", \"zurich\", 17, 23), (\"city\", \"bern\", 24, 28), (\"city\", \"basel\", 29, 34), (\"city\", \"winterthur\", 35, 46), (\"city\", \"lucerne\", 47, 54), (\"city\", \"st. gallen\", 55, 65), (\"city\", \"lugano\", 66, 72), (\"city\", \"biel/bienne\", 73, 85)])\n",
    "\n",
    "    _table_features_sample_vectors = f\"\"\"\n",
    "        INSERT INTO feature (feature_type, version, model_name, model_params, data, media_id, embedding_size, embedding_1024)\n",
    "        VALUES\n",
    "            ('{feature_types[i % len(feature_types)]}', 'v1', 'resnet50', '{{\"param1\": \"{i}\"}}', '{{\"color\": \"{color_meta[i % len(color_meta)] }\", \"data1\": \"{i}\", \"ner\": {ner_tags} }}', {i}, 1024, ARRAY[{','.join([str(x) for x in vector_1024])}])\n",
    "        RETURNING feature_id;\n",
    "    \"\"\"\n",
    "    # print(_table_features_sample_vectors)\n",
    "    execute_write_query(_table_features_sample_vectors)\n",
    "\n",
    "    # At the moment we are only creating size 1024 vectors, for the sake of the next example queries to work, there can be only a single vector set per feature\n",
    "    # _table_features_sample_vectors = f\"\"\"\n",
    "    #     INSERT INTO features (feature_type, version, model_name, model_params, data, media_id, embedding_size, embedding_2048)\n",
    "    #     VALUES\n",
    "    #         ('image', 'v1', 'resnet50', '{{\"param1\": \"{i}\"}}', '{{\"data1\": \"{i}\"}}', {i}, 2048, ARRAY[{','.join([str(x) for x in vector_2048])}])\n",
    "    #     RETURNING feature_id;\n",
    "    # \"\"\"\n",
    "\n",
    "    # execute_write_query(_table_features_sample_vectors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "### Scenario 1\n",
    "\n",
    "We have a media object and we want to find the 5 most similar media objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 12.6693015514196),\n",
       " (8, 12.880733172073),\n",
       " (4, 12.881589969474),\n",
       " (7, 12.9209399099366),\n",
       " (1, 12.967205309647)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"\"\"\n",
    "    WITH target_embedding AS (\n",
    "    SELECT\n",
    "        media_id,\n",
    "        CASE\n",
    "            WHEN embedding_size = 1024 THEN embedding_1024\n",
    "            WHEN embedding_size = 2048 THEN embedding_2048\n",
    "            ELSE NULL\n",
    "        END AS embedding_vector\n",
    "    FROM \n",
    "        feature\n",
    "    WHERE \n",
    "        media_id = 5\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "    f.media_id,\n",
    "    (target.embedding_vector <-> \n",
    "        CASE\n",
    "        WHEN f.embedding_size = 1024 THEN f.embedding_1024\n",
    "        WHEN f.embedding_size = 2048 THEN f.embedding_2048\n",
    "        ELSE NULL\n",
    "        END\n",
    "    ) AS distance\n",
    "    FROM\n",
    "    feature f,\n",
    "    target_embedding target\n",
    "    WHERE\n",
    "    f.media_id != target.media_id\n",
    "    ORDER BY\n",
    "    distance ASC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "\n",
    "Find all media objects for Zurich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '/path/to/media1',\n",
       "  '/path/to/original1',\n",
       "  datetime.datetime(2023, 5, 4, 12, 6, 17, 657201),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  {'city': 'Zurich', 'key1': 'value1', 'year': 2015},\n",
       "  1,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None),\n",
       " (11,\n",
       "  '/path/to/media11',\n",
       "  '/path/to/original11',\n",
       "  datetime.datetime(2023, 5, 4, 12, 6, 18, 143418),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  {'city': 'Zurich', 'year': 2019, 'key11': 'value11'},\n",
       "  1,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all media objects that have city: \"Zurich\" (queried from the jsonb metadata field)\n",
    "_query = \"\"\"\n",
    "    SELECT * FROM media WHERE metadata->>'city' = 'Zurich';\n",
    "\"\"\"\n",
    "execute_read_query(_query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3\n",
    "\n",
    "Get all images from Geneva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  '/path/to/media2',\n",
       "  '/path/to/original2',\n",
       "  datetime.datetime(2023, 5, 4, 12, 6, 17, 697058),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  {'city': 'Geneva', 'key2': 'value2', 'year': 2016},\n",
       "  1,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None),\n",
       " (12,\n",
       "  '/path/to/media12',\n",
       "  '/path/to/original12',\n",
       "  datetime.datetime(2023, 5, 4, 12, 6, 18, 195346),\n",
       "  'image',\n",
       "  'jpg',\n",
       "  500,\n",
       "  {'city': 'Geneva', 'year': 2020, 'key12': 'value12'},\n",
       "  1,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"\"\"\n",
    "    SELECT * FROM media WHERE metadata->>'city' = 'Geneva' AND media_type = 'image';\n",
    "\"\"\"\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 4\n",
    "\n",
    "Fulltext string matching on jsonb fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Basel', 'Bern', 'Biel/Bienne', 'Basel', 'Bern', 'Biel/Bienne']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['St. Gallen', 'St. Gallen']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query for cities that start with b\n",
    "_query = \"\"\"\n",
    "    SELECT metadata FROM media WHERE metadata->>'city' LIKE 'B%';\n",
    "\"\"\"\n",
    "print([x[0]['city'] for x in execute_read_query(_query)])\n",
    "\n",
    "_query = \"\"\"\n",
    "    SELECT metadata FROM media WHERE metadata->>'city' LIKE '%Gall%';\n",
    "\"\"\"\n",
    "[x[0]['city'] for x in execute_read_query(_query)]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 5\n",
    "\n",
    "Similarity to a computed vector. Example: we have a video camera installed and a user poses like a tennis player and we find tennis matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 13.0702585003252), (4, 13.2722672142341)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_1024 = \",\".join([str(x) for x in np.random.rand(1024).tolist()])  # feature that would be creating by the pose detection algorithm\n",
    "\n",
    "# query the feature table for similar vectors\n",
    "_query = f\"\"\"\n",
    "    SELECT\n",
    "    f.media_id,\n",
    "    ('[{vector_1024}]' <-> \n",
    "        CASE\n",
    "        WHEN f.embedding_size = 1024 THEN f.embedding_1024\n",
    "        WHEN f.embedding_size = 2048 THEN f.embedding_2048\n",
    "        ELSE NULL\n",
    "        END\n",
    "    ) AS distance\n",
    "    FROM\n",
    "    feature f\n",
    "    WHERE\n",
    "    f.feature_type = 'pose'\n",
    "    ORDER BY\n",
    "    distance ASC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 6\n",
    "\n",
    "Find people or locations with ner tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'green',\n",
       "   'data1': '1'}),\n",
       " (2,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'blue',\n",
       "   'data1': '2'}),\n",
       " (3,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'yellow',\n",
       "   'data1': '3'}),\n",
       " (4,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'black',\n",
       "   'data1': '4'}),\n",
       " (5,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'white',\n",
       "   'data1': '5'}),\n",
       " (6,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'grey',\n",
       "   'data1': '6'}),\n",
       " (7,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'orange',\n",
       "   'data1': '7'}),\n",
       " (8,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'purple',\n",
       "   'data1': '8'}),\n",
       " (9,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'pink',\n",
       "   'data1': '9'}),\n",
       " (10,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'red',\n",
       "   'data1': '10'})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person = \"Ueli Steck\"\n",
    "\n",
    "_query = f\"\"\"\n",
    "    SELECT media_id, data FROM feature WHERE data->>'ner' LIKE '%{person}%';\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 7\n",
    "\n",
    "Find aribitrary features by metadata (here we use the simple field color as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10,\n",
       "  {'ner': [['person', 'Ueli Steck', 1, 9],\n",
       "    ['city', 'geneva', 10, 16],\n",
       "    ['city', 'zurich', 17, 23],\n",
       "    ['city', 'bern', 24, 28],\n",
       "    ['city', 'basel', 29, 34],\n",
       "    ['city', 'winterthur', 35, 46],\n",
       "    ['city', 'lucerne', 47, 54],\n",
       "    ['city', 'st. gallen', 55, 65],\n",
       "    ['city', 'lugano', 66, 72],\n",
       "    ['city', 'biel/bienne', 73, 85]],\n",
       "   'color': 'red',\n",
       "   'data1': '10'})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color = \"red\"\n",
    "\n",
    "_query = f\"\"\"\n",
    "    SELECT media_id, data FROM feature WHERE data->>'color' = '{color}';\n",
    "\"\"\"\n",
    "\n",
    "execute_read_query(_query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest the clips to the media table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOTE_RTS_DATA = \"/media/sinergia/RTS/\"\n",
    "REMOTE_VIDEOS = '/mnt/rts/'\n",
    "\n",
    "LOCAL_RTS_DATA = \"/media/data/rts/\"\n",
    "METADATA = LOCAL_RTS_DATA + 'metadata'\n",
    "LOCAL_VIDEOS = LOCAL_RTS_DATA + 'archive'\n",
    "\n",
    "AIBOX = LOCAL_RTS_DATA + 'aibox-vectors'\n",
    "\n",
    "OUTDIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import hashlib\n",
    "from supabase import create_client, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/rts/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# LOCAL imports\n",
    "import emv\n",
    "import emv.pipeline\n",
    "import emv.utils\n",
    "import emv.io.media\n",
    "import emv.features.audio\n",
    "import emv.features.text\n",
    "\n",
    "LOG = emv.utils.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase: Client = create_client(\n",
    "    os.getenv(\"SUPABASE_HOST\"), \n",
    "    os.getenv(\"SUPABASE_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"rts\"\n",
    "# res = supabase.storage.create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3177, 22)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = emv.utils.dataframe_from_hdf5(LOCAL_RTS_DATA + '/metadata', 'rts_aivectors')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the clips from the archive and put them into supabase s3. At the same time create the database entries on the media table\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def ingest_clips(df: pd.DataFrame, prefix_name: str, supabase: Client, bucket_name: str, library_id: int) -> Tuple[int, int]:\n",
    "    error_count = 0\n",
    "    no_clips = 0\n",
    "    default_format = \"mp4\"\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        base_path = os.path.join(row.mediaFolderPath.replace(REMOTE_VIDEOS, LOCAL_VIDEOS + '/'))\n",
    "        path = os.path.join(base_path, 'clips', 'videos')\n",
    "\n",
    "        # get all files in the folder\n",
    "        try:\n",
    "            files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "            no_clips += len(files)\n",
    "\n",
    "            with open(os.path.join(base_path, \"clips.json\")) as f:\n",
    "                clip_metadata = orjson.loads(f.read())\n",
    "\n",
    "            metadata = {\n",
    "                \"image_resolutions\": clip_metadata['image_resolutions'],\n",
    "                \"framerate\": clip_metadata['framerate'],\n",
    "                \"clip_count\": clip_metadata['clip_count'],\n",
    "            }\n",
    "\n",
    "            # TODO: Create parent media file for the source video for clips\n",
    "            parent_id = 1  # TODO: get the parent id from the database\n",
    "            original_path = f\"{base_path}.{default_format}\"\n",
    "            hash = hashlib.md5((original_path + str(0) + str(0)).encode('utf-8')).hexdigest()\n",
    "\n",
    "            write_media_object_db(\n",
    "                media_path=f\"{prefix_name}/{row.mediaFolderPath.replace(REMOTE_VIDEOS, '')}.{default_format}\",\n",
    "                original_path=original_path,\n",
    "                library_id=library_id,\n",
    "                parent_id=0,\n",
    "                update_data=json.dumps(metadata),\n",
    "                media_type='video',\n",
    "                media_sub_type='source_video',\n",
    "                frame_rate=clip_metadata['framerate'],\n",
    "                hash=hash,\n",
    "            )\n",
    "\n",
    "            # upload all files to supabase and create the database entries\n",
    "            for f in files:\n",
    "                supabase_key = f\"{prefix_name}/{row.mediaFolderPath.replace(REMOTE_VIDEOS, '')}/clips/videos/{f}\"\n",
    "                file_path = os.path.join(path, files[0])\n",
    "\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                \n",
    "                clip_name = f.split('.')[0]\n",
    "                start_ts = clip_metadata['clips'][clip_name]['start']\n",
    "                end_ts = clip_metadata['clips'][clip_name]['end']\n",
    "                start_frame = clip_metadata['clips'][clip_name]['start_frame']\n",
    "                end_frame = clip_metadata['clips'][clip_name]['end_frame']\n",
    "                frame_rate = clip_metadata['framerate']\n",
    "\n",
    "                hash = hashlib.md5((file_path + str(start_frame) + str(end_frame)).encode('utf-8')).hexdigest()\n",
    "\n",
    "                update_data = json.dumps(metadata | {'ref_text': clip_metadata['clips'][clip_name]['ref_text'], 'locations': clip_metadata['clips'][clip_name]['locations']})\n",
    "                update_data = update_data.replace(\"'\", \"''\")\n",
    "\n",
    "                write_media_object_db(\n",
    "                    media_path=supabase_key,\n",
    "                    original_path=os.path.abspath(file_path),\n",
    "                    library_id=library_id,\n",
    "                    parent_id=parent_id,\n",
    "                    start_ts=start_ts,\n",
    "                    end_ts=end_ts,\n",
    "                    start_frame=start_frame,\n",
    "                    end_frame=end_frame,\n",
    "                    frame_rate=frame_rate,\n",
    "                    update_data=update_data,\n",
    "                    file_size=file_size,\n",
    "                    hash=hash,\n",
    "                    media_type='video',\n",
    "                    media_sub_type='clip'\n",
    "                )\n",
    "\n",
    "                # upload to supabase s3\n",
    "                # supabase.storage.from_(bucket_name).upload(supabase_key, os.path.abspath(file_path))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"No clips folund for {error_count} rows\")\n",
    "    print(f\"Total number of clips: {no_clips}\")\n",
    "    print(f\"Total number of vidoes with clips: {len(df) - error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 146/3177 [00:05<01:54, 26.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m bucket_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrts\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m library_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m r \u001b[39m=\u001b[39m ingest_clips(df, \u001b[39m\"\u001b[39;49m\u001b[39mtest4\u001b[39;49m\u001b[39m\"\u001b[39;49m, supabase, bucket_name, library_id)\n",
      "Cell \u001b[0;32mIn[48], line 66\u001b[0m, in \u001b[0;36mingest_clips\u001b[0;34m(df, prefix_name, supabase, bucket_name, library_id)\u001b[0m\n\u001b[1;32m     63\u001b[0m         update_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(metadata \u001b[39m|\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mref_text\u001b[39m\u001b[39m'\u001b[39m: clip_metadata[\u001b[39m'\u001b[39m\u001b[39mclips\u001b[39m\u001b[39m'\u001b[39m][clip_name][\u001b[39m'\u001b[39m\u001b[39mref_text\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mlocations\u001b[39m\u001b[39m'\u001b[39m: clip_metadata[\u001b[39m'\u001b[39m\u001b[39mclips\u001b[39m\u001b[39m'\u001b[39m][clip_name][\u001b[39m'\u001b[39m\u001b[39mlocations\u001b[39m\u001b[39m'\u001b[39m]})\n\u001b[1;32m     64\u001b[0m         update_data \u001b[39m=\u001b[39m update_data\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m         write_media_object_db(\n\u001b[1;32m     67\u001b[0m             media_path\u001b[39m=\u001b[39;49msupabase_key,\n\u001b[1;32m     68\u001b[0m             original_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(file_path),\n\u001b[1;32m     69\u001b[0m             library_id\u001b[39m=\u001b[39;49mlibrary_id,\n\u001b[1;32m     70\u001b[0m             parent_id\u001b[39m=\u001b[39;49mparent_id,\n\u001b[1;32m     71\u001b[0m             start_ts\u001b[39m=\u001b[39;49mstart_ts,\n\u001b[1;32m     72\u001b[0m             end_ts\u001b[39m=\u001b[39;49mend_ts,\n\u001b[1;32m     73\u001b[0m             start_frame\u001b[39m=\u001b[39;49mstart_frame,\n\u001b[1;32m     74\u001b[0m             end_frame\u001b[39m=\u001b[39;49mend_frame,\n\u001b[1;32m     75\u001b[0m             frame_rate\u001b[39m=\u001b[39;49mframe_rate,\n\u001b[1;32m     76\u001b[0m             update_data\u001b[39m=\u001b[39;49mupdate_data,\n\u001b[1;32m     77\u001b[0m             file_size\u001b[39m=\u001b[39;49mfile_size,\n\u001b[1;32m     78\u001b[0m             \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[1;32m     79\u001b[0m             media_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvideo\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     80\u001b[0m             media_sub_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclip\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     83\u001b[0m         \u001b[39m# upload to supabase s3\u001b[39;00m\n\u001b[1;32m     84\u001b[0m         \u001b[39m# supabase.storage.from_(bucket_name).upload(supabase_key, os.path.abspath(file_path))\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/rts/rts/db/utils.py:63\u001b[0m, in \u001b[0;36mwrite_media_object_db\u001b[0;34m(media_path, original_path, library_id, parent_id, start_ts, end_ts, start_frame, end_frame, frame_rate, update_data, file_size, hash, media_type, media_sub_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_media_object_db\u001b[39m(\n\u001b[1;32m     35\u001b[0m         media_path: \u001b[39mstr\u001b[39m, \n\u001b[1;32m     36\u001b[0m         original_path: \u001b[39mstr\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m         media_type: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     48\u001b[0m         media_sub_type: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     49\u001b[0m     _query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[39m        INSERT INTO media (\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[39m            media_path, original_path, media_type, sub_type, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 63\u001b[0m     execute_write_query(_query)\n",
      "File \u001b[0;32m~/rts/rts/db/utils.py:31\u001b[0m, in \u001b[0;36mexecute_write_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mwith\u001b[39;00m conn\u001b[39m.\u001b[39mcursor() \u001b[39mas\u001b[39;00m cur:\n\u001b[1;32m     30\u001b[0m     cur\u001b[39m.\u001b[39mexecute(query)\n\u001b[0;32m---> 31\u001b[0m conn\u001b[39m.\u001b[39;49mcommit()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bucket_name = \"rts\"\n",
    "library_id = \"1\"\n",
    "\n",
    "r = ingest_clips(df, \"test4\", supabase, bucket_name, library_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
