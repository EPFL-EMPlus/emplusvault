{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing Locations from extracted text\n",
    "\n",
    "Different methods to get the location the RTS broadcasts are about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emv.db.dao import DataAccessObject\n",
    "from sqlalchemy.sql import text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "\n",
    "from emv.client.get_content import get_features\n",
    "from emv.utils import dataframe_from_hdf5\n",
    "from emv.features.wikidata import get_wikidata_id, get_property, get_wikidata_label\n",
    "from emv.features.wikidata import process_batch\n",
    "from emv.settings import DRIVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = text(\"\"\"SELECT * FROM feature WHERE feature_type = 'transcript+ner';\"\"\")\n",
    "df = pd.DataFrame(DataAccessObject().fetch_all(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_entities = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for ent in row['data']['entities']:\n",
    "            if ent[1] == 'LOC':\n",
    "                loc_entities.append(ent[0].lower())\n",
    "                # print(ent['text'])\n",
    "        # print(row['data']['entities'])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(loc_entities).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_locs = []\n",
    "for s in series.items():\n",
    "    all_locs.append(s)\n",
    "print(all_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = []\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for t in row['data']['transcript']:\n",
    "            streets.append(t['t'])\n",
    "        # row['data']['transcript'][0]['t']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for t in row['data']['transcript']:\n",
    "            if 'rue de' in t['t'].lower():\n",
    "                print(t['t'])\n",
    "        # row['data']['transcript'][0]['t']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/geneva_streets.txt\", \"r\") as f:\n",
    "    streets = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_street(sent):\n",
    "    corrected_street = sent.replace(\"ruedes\", \"rue des\")\n",
    "    corrected_street = corrected_street.replace(\"cheminde\", \"chemin de\")\n",
    "    corrected_street = corrected_street.replace(\"placedes\", \"place des\")\n",
    "    corrected_street = corrected_street.replace(\"placede\", \"place de\")\n",
    "    corrected_street = corrected_street.replace(\"routedes\", \"route des\")\n",
    "    corrected_street = corrected_street.replace(\"avenuede\", \"avenue de\")\n",
    "    corrected_street = corrected_street.replace(\"avenuedu\", \"avenue du\")\n",
    "    corrected_street = corrected_street.replace(\"ruede\", \"rue de\")\n",
    "    corrected_street = corrected_street.replace(\"quaidu\", \"quai du\")\n",
    "    corrected_street = corrected_street.replace(\"placedu\", \"place du\")\n",
    "    corrected_street = corrected_street.replace(\"promenadedu\", \"promenade du\")\n",
    "    corrected_street = corrected_street.replace(\"ruedu\", \"rue du\")\n",
    "    corrected_street = corrected_street.replace(\"routede\", \"route de\")\n",
    "    corrected_street = corrected_street.replace(\"passagedes\", \"passage des\")\n",
    "    corrected_street = corrected_street.replace(\"chemindes\", \"chemin des\")\n",
    "    corrected_street = corrected_street.replace(\"ruedes\", \"rue des\")\n",
    "    corrected_street = corrected_street.replace(\"squaredu\", \"square du\")\n",
    "    corrected_street = corrected_street.replace(\"passagede\", \"passage de\")\n",
    "    corrected_street = corrected_street.replace(\"promenade des\", \"promenade des \")\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load the French language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Initialize the Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "street_names = [\"Rue\", \"Chemin\", \"Place\", \"Avenue\", \"Boulevard\", \"Quai\", \"Promenade\", \"Route\", \"Square\"]\n",
    "connectors = [\"des\", \"de\", \"du\", \"la\", \"le\", \"les\", \"l'\", \"d'\", \"au\", \"aux\"]\n",
    "second_connectors = [\"l'\", \"d'\", \"la\"]\n",
    "\n",
    "# Define the pattern\n",
    "pattern = [\n",
    "    {\"TEXT\": {\"IN\": street_names}},\n",
    "    {\"TEXT\": {\"IN\": connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"REGEX\": \"^[a-zA-Z'-]+$\"}, \"OP\": \"+\"},\n",
    "]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"ADDRESS\", [pattern])\n",
    "\n",
    "# create a second pattern to match things like Rue de l'Hôtel-de-Ville or Chemin de la Gravière\n",
    "pattern2 = [\n",
    "    {\"TEXT\": {\"IN\": street_names}},\n",
    "    {\"TEXT\": {\"IN\": connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"IN\": second_connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"REGEX\": \"^[a-zA-Z-ôèéê]+$\"}, \"OP\": \"+\"},\n",
    "\n",
    "]\n",
    "matcher.add(\"ADDRESS2\", [pattern2])\n",
    "\n",
    "# Process the sentences and get the matches\n",
    "matched_streets = []\n",
    "\n",
    "for street in streets:\n",
    "    street = replace_street(street)\n",
    "    # sentence = f\"Hier, un accident s'est produit dans le sud de Genève, {street}, et 5 personnes ont été blessées. Le trafic est perturbé dans le secteur.\"\n",
    "    sentence = street\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # if matches are overlapped, we only keep the longest one\n",
    "    if len(matches) > 1:\n",
    "        matches = sorted(matches, key=lambda x: x[2]-x[1], reverse=True)\n",
    "        matches = [matches[0]]\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        matched_streets.append(span.text)\n",
    "\n",
    "matched_streets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(feature_type='transcript+ner', max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = dataframe_from_hdf5(DRIVE_PATH + \"rts/metadata\", \"rts_metadata\")\n",
    "metadata.reset_index(inplace=True)\n",
    "metadata.rename(columns = {\"mediaId\": \"rts_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rts_id\"] = df[\"media_id\"].apply(lambda x: x.split(\"-\")[1])\n",
    "df = df.merge(metadata, on='rts_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = metadata.drop_duplicates(subset=\"rts_id\").mediaDuration.sum()\n",
    "processed_duration = df.drop_duplicates(subset=\"rts_id\").mediaDuration.sum()\n",
    "\n",
    "print(f\"Processed {100 * processed_duration / total_duration:.2f}% of the total duration\")\n",
    "print(f\"Processed {len(df.rts_id.unique())} videos out of {len(metadata.rts_id.unique())} - {len(df.rts_id.unique())/len(metadata.rts_id.unique())*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"publishedDate\"] = pd.to_datetime(df[\"publishedDate\"])\n",
    "df[\"year\"] = df[\"publishedDate\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "df.groupby(\"rts_id\").categoryName.agg(set).map(lambda x: list(x)[0]).value_counts().plot(kind=\"barh\")\n",
    "plt.title(\"Number of videos per category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "df.rts_id.value_counts().hist(bins=100, grid = False)\n",
    "plt.title(\"Number of clips extracted per video\")\n",
    "plt.text(100, 2200, f\"{len(df)} clips extracted\\nout of {len(df.rts_id.unique())} videos\", fontdict={\"size\": 12, \"weight\": \"bold\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "df.groupby(\"rts_id\").year.mean().hist(bins=50, ax=axs[0], grid = False)\n",
    "axs[0].set_title(\"Distribution of the year of publication of the videos\")\n",
    "df.year.hist(bins=50, ax=axs[1], grid = False)\n",
    "axs[1].set_title(\"Distribution of the year of publication of the clips\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(data):\n",
    "    entities = []\n",
    "    if \"entities\" in data.keys():\n",
    "        entities = data[\"entities\"]\n",
    "    else:\n",
    "        entities = [t.get(\"entities\", []) for t in data.get(\"transcript\", [])]\n",
    "        entities = [e for sublist in entities for e in sublist]\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entities\"] = df[\"data\"].apply(get_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types = list(set([e[1] for sublist in df.entities for e in sublist]))\n",
    "entity_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"locations\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"LOC\" and len(e[0]) > 2])\n",
    "df[\"people\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"PER\" and len(e[0]) > 2])\n",
    "df[\"orgs\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"ORG\" and len(e[0]) > 1])\n",
    "df[\"misc\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"MISC\" and len(e[0]) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"media_id\", \"rts_id\", \"year\", \"categoryName\", \"contentType\", \"title\", \"resume\", \"mediaDuration\", \"locations\", \"people\", \"orgs\", \"misc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/rts_transcript_ner.csv\", index=False, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load presaved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/rts_transcript_ner.csv\", sep = \"\\t\", converters={\"locations\": literal_eval,\n",
    "                                                                           \"people\": literal_eval,\n",
    "                                                                           \"orgs\": literal_eval,\n",
    "                                                                           \"misc\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = metadata.drop_duplicates(subset=\"rts_id\").mediaDuration.sum()\n",
    "processed_duration = df.drop_duplicates(subset=\"rts_id\").mediaDuration.sum()\n",
    "print(\"Hours processed:\", processed_duration / 3600)\n",
    "print(f\"Processed {100 * processed_duration / total_duration :.03f}% of the total duration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map of Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[\"locations\"].explode().value_counts()\n",
    "locations = pd.DataFrame(locations).reset_index().rename(columns={\"locations\":\"location\"})\n",
    "locations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[[\"locations\", \"year\"]].explode(\"locations\").dropna().groupby(\"locations\").agg(list).reset_index()\n",
    "locations[\"count\"] = locations[\"year\"].apply(len)\n",
    "locations[\"year\"] = locations.year.map(lambda x: Counter(x))\n",
    "locations = locations.sort_values(\"count\", ascending=False)\n",
    "locations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../emv/features/cities.json\", \"r\") as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame([{\"locations\":k, \"lon\":float(v[0]), \"lat\":float(v[1])} for k,v in cities.items() if len(v) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities = pd.merge(locations, cities, on=\"locations\", how=\"left\").dropna(subset = [\"lat\", \"lon\"])\n",
    "found_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base map centered around Switzerland\n",
    "m = folium.Map(location=[46.8182, 8.2275], zoom_start=8)\n",
    "size_multiplier = 1\n",
    "# Add city points to the map\n",
    "for index, row in found_cities.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lon'], row['lat']),\n",
    "        radius=np.sqrt(row['count'] / np.pi) * size_multiplier,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        tooltip=row['locations'] + ': ' + str(row['count']) + ' occurrences'\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mentions_per_year = found_cities.year.sum()\n",
    "n_mentions_per_year = pd.DataFrame(n_mentions_per_year.items(), columns=[\"year\", \"count\"])\n",
    "n_mentions_per_year = n_mentions_per_year.sort_values(\"year\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.lineplot(data=n_mentions_per_year, x=\"year\", y=\"count\")\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mentions_per_year_dict = n_mentions_per_year.set_index(\"year\").to_dict()[\"count\"]\n",
    "found_cities[\"relative_counts\"] = found_cities.year.map(lambda x: {k:v / n_mentions_per_year_dict[k] for k,v in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "found_cities[:20].sort_values(\"count\").set_index(\"locations\")[\"count\"].plot(kind=\"barh\")\n",
    "plt.xlabel(\"Number of mentions\")\n",
    "plt.title(\"Top 20 locations mentioned in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 20\n",
    "skip_first_n = 1\n",
    "top_cities = found_cities[skip_first_n:top_N + skip_first_n]\n",
    "counts_per_year = pd.DataFrame(top_cities.year.tolist(), index=top_cities.locations).fillna(0)\n",
    "counts_per_year = counts_per_year.T.sort_index().T\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(counts_per_year, cmap=\"Blues\", cbar_kws={'label': 'Number of mentions'})\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for loc,mentions in counts_per_year.iterrows():\n",
    "    plt.plot(mentions.index, mentions.values, label=loc)\n",
    "plt.legend()\n",
    "plt.title(f\"Number of mentions of the top {len(counts_per_year)} locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 20\n",
    "skip_first_n = 1\n",
    "top_cities = found_cities[skip_first_n:top_N + skip_first_n]\n",
    "counts_per_year = pd.DataFrame(top_cities.relative_counts.tolist(), index=top_cities.locations).fillna(0)\n",
    "counts_per_year = counts_per_year.T.sort_index().T\n",
    "\n",
    "counts_per_year = counts_per_year.div(counts_per_year.sum(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(counts_per_year, cmap=\"Blues\", cbar_kws={'label': 'Number of mentions'})\n",
    "plt.title(\"Number of mentions of locations in Switzerland in the RTS videos\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for loc,mentions in counts_per_year.iterrows():\n",
    "    plt.plot(mentions.index, mentions.values, label=loc)\n",
    "plt.legend()\n",
    "plt.title(f\"Number of mentions of the top {len(counts_per_year)} locations in Switzerland in the RTS videos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_people = [\"messieurs\", \"monsieur\", \"madame\", \"mesdames\"]\n",
    "\n",
    "df[\"people\"] = df[\"people\"].apply(lambda x: [p for p in x if p.lower() not in filter_people])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = df[[\"people\", \"year\"]].explode(\"people\").dropna().groupby(\"people\").agg(list).reset_index()\n",
    "persons[\"count\"] = persons[\"year\"].apply(len)\n",
    "persons[\"year\"] = persons.year.map(lambda x: Counter(x))\n",
    "persons = persons.sort_values(\"count\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = persons.head(1000)\n",
    "top_persons.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"wikidata_search\"] = top_persons[\"people\"].apply(get_wikidata_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = top_persons[top_persons[\"wikidata_search\"].map(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"wikidata_id\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"id\", \"MISSING_ID\") if len(x) > 0 else None)\n",
    "top_persons[\"wikidata_label\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"label\", \"MISSING_LABEL\") if len(x) > 0 else None)\n",
    "top_persons[\"wikidata_description\"] = top_persons[\"wikidata_search\"].apply(lambda x: x[0].get(\"description\", \"MISSING_DESCRIPTION\") if len(x) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_of = process_batch(top_persons[\"wikidata_id\"].dropna().tolist(), \"P31\", BATCH_SIZE=20)\n",
    "top_persons[\"instance_of\"] = top_persons[\"wikidata_id\"].map(instance_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = top_persons.instance_of.dropna().unique().tolist()\n",
    "instances = {i:get_wikidata_label(i.split(\"/\")[-1]) for i in instances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"instance_of\"] = top_persons[\"instance_of\"].map(instances)\n",
    "top_persons[\"instance_of\"] = top_persons[\"instance_of\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"instance_of\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on humans (instance of Q5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons.dropna(subset=[\"wikidata_id\", \"instance_of\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons = top_persons[top_persons.instance_of == \"human\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = top_persons[\"wikidata_id\"].tolist()\n",
    "citizenship = process_batch(ids, \"P27\", BATCH_SIZE=20)\n",
    "top_persons[\"citizenship\"] = top_persons[\"wikidata_id\"].map(citizenship)\n",
    "\n",
    "occupation = process_batch(ids, \"P106\", BATCH_SIZE=20)\n",
    "top_persons[\"occupation\"] = top_persons[\"wikidata_id\"].map(occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citizenship_labels = {k:get_wikidata_label(k.split(\"/\")[-1]) for k in top_persons[\"citizenship\"].dropna().unique()}\n",
    "top_persons[\"citizenship\"] = top_persons[\"citizenship\"].map(citizenship_labels)\n",
    "\n",
    "occupation_labels = {k:get_wikidata_label(k.split(\"/\")[-1]) for k in top_persons[\"occupation\"].dropna().unique()}\n",
    "top_persons[\"occupation\"] = top_persons[\"occupation\"].map(occupation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_persons[\"citizenship\"] = top_persons[\"citizenship\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")\n",
    "top_persons[\"occupation\"] = top_persons[\"occupation\"].fillna(\"MISSING\").map(lambda x: x.get(\"en\", {}).get(\"value\", \"MISSING\") if x != \"MISSING\" else \"MISSING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "top_persons.occupation.value_counts()[:20].plot(kind=\"barh\", ax=axs[0])\n",
    "axs[0].set_title(\"Occupations of most occurring persons\")\n",
    "top_persons.citizenship.value_counts()[:20].plot(kind=\"barh\", ax=axs[1])\n",
    "axs[1].set_title(\"Citizenships of most occurring persons\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = pd.read_csv(\"/mnt/g/rts/aibox-vectors/videos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.rename(columns = {\"umid\": \"rts_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.merge(metadata, on=\"rts_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_sample = videos.rts_id.tolist()\n",
    "print(len(representative_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.rts_id.isin(representative_sample)].rts_id.unique().tolist()) / len(representative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rts-bWoRmFur-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
