{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing Locations from extracted text\n",
    "\n",
    "Different methods to get the location the RTS broadcasts are about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emv.db.dao import DataAccessObject\n",
    "from sqlalchemy.sql import text\n",
    "import pandas as pd\n",
    "import json\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = text(\"\"\"SELECT * FROM feature WHERE feature_type = 'transcript+ner';\"\"\")\n",
    "df = pd.DataFrame(DataAccessObject().fetch_all(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_entities = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for ent in row['data']['entities']:\n",
    "            if ent[1] == 'LOC':\n",
    "                loc_entities.append(ent[0].lower())\n",
    "                # print(ent['text'])\n",
    "        # print(row['data']['entities'])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(loc_entities).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_locs = []\n",
    "for s in series.items():\n",
    "    all_locs.append(s)\n",
    "print(all_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = []\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for t in row['data']['transcript']:\n",
    "            streets.append(t['t'])\n",
    "        # row['data']['transcript'][0]['t']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for t in row['data']['transcript']:\n",
    "            if 'rue de' in t['t'].lower():\n",
    "                print(t['t'])\n",
    "        # row['data']['transcript'][0]['t']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/geneva_streets.txt\", \"r\") as f:\n",
    "    streets = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_street(sent):\n",
    "    corrected_street = sent.replace(\"ruedes\", \"rue des\")\n",
    "    corrected_street = corrected_street.replace(\"cheminde\", \"chemin de\")\n",
    "    corrected_street = corrected_street.replace(\"placedes\", \"place des\")\n",
    "    corrected_street = corrected_street.replace(\"placede\", \"place de\")\n",
    "    corrected_street = corrected_street.replace(\"routedes\", \"route des\")\n",
    "    corrected_street = corrected_street.replace(\"avenuede\", \"avenue de\")\n",
    "    corrected_street = corrected_street.replace(\"avenuedu\", \"avenue du\")\n",
    "    corrected_street = corrected_street.replace(\"ruede\", \"rue de\")\n",
    "    corrected_street = corrected_street.replace(\"quaidu\", \"quai du\")\n",
    "    corrected_street = corrected_street.replace(\"placedu\", \"place du\")\n",
    "    corrected_street = corrected_street.replace(\"promenadedu\", \"promenade du\")\n",
    "    corrected_street = corrected_street.replace(\"ruedu\", \"rue du\")\n",
    "    corrected_street = corrected_street.replace(\"routede\", \"route de\")\n",
    "    corrected_street = corrected_street.replace(\"passagedes\", \"passage des\")\n",
    "    corrected_street = corrected_street.replace(\"chemindes\", \"chemin des\")\n",
    "    corrected_street = corrected_street.replace(\"ruedes\", \"rue des\")\n",
    "    corrected_street = corrected_street.replace(\"squaredu\", \"square du\")\n",
    "    corrected_street = corrected_street.replace(\"passagede\", \"passage de\")\n",
    "    corrected_street = corrected_street.replace(\"promenade des\", \"promenade des \")\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load the French language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Initialize the Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "street_names = [\"Rue\", \"Chemin\", \"Place\", \"Avenue\", \"Boulevard\", \"Quai\", \"Promenade\", \"Route\", \"Square\"]\n",
    "connectors = [\"des\", \"de\", \"du\", \"la\", \"le\", \"les\", \"l'\", \"d'\", \"au\", \"aux\"]\n",
    "second_connectors = [\"l'\", \"d'\", \"la\"]\n",
    "\n",
    "# Define the pattern\n",
    "pattern = [\n",
    "    {\"TEXT\": {\"IN\": street_names}},\n",
    "    {\"TEXT\": {\"IN\": connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"REGEX\": \"^[a-zA-Z'-]+$\"}, \"OP\": \"+\"},\n",
    "]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"ADDRESS\", [pattern])\n",
    "\n",
    "# create a second pattern to match things like Rue de l'Hôtel-de-Ville or Chemin de la Gravière\n",
    "pattern2 = [\n",
    "    {\"TEXT\": {\"IN\": street_names}},\n",
    "    {\"TEXT\": {\"IN\": connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"IN\": second_connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"REGEX\": \"^[a-zA-Z-ôèéê]+$\"}, \"OP\": \"+\"},\n",
    "\n",
    "]\n",
    "matcher.add(\"ADDRESS2\", [pattern2])\n",
    "\n",
    "# Process the sentences and get the matches\n",
    "matched_streets = []\n",
    "\n",
    "for street in streets:\n",
    "    street = replace_street(street)\n",
    "    # sentence = f\"Hier, un accident s'est produit dans le sud de Genève, {street}, et 5 personnes ont été blessées. Le trafic est perturbé dans le secteur.\"\n",
    "    sentence = street\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # if matches are overlapped, we only keep the longest one\n",
    "    if len(matches) > 1:\n",
    "        matches = sorted(matches, key=lambda x: x[2]-x[1], reverse=True)\n",
    "        matches = [matches[0]]\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        matched_streets.append(span.text)\n",
    "\n",
    "matched_streets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../emv/features/cities.json\", \"r\") as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name = series.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_clips = []\n",
    "for key in cities:\n",
    "    if key.lower() in city_name:\n",
    "        # print(key, series[series.index == key.lower()].values[0])\n",
    "        found_clips.append({\n",
    "            \"city\": key, \n",
    "            \"no_occurrences\": series[series.index == key.lower()].values[0], \n",
    "            \"lon\": cities[key][0], \"lat\": cities[key][1]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(found_clips).sort_values(by=\"no_occurrences\", ascending=False).reset_index()[['city', 'no_occurrences', 'lon', 'lat']].to_csv(\"data/cities.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.DataFrame(found_clips).sort_values(by=\"no_occurrences\", ascending=False).reset_index()[['city', 'no_occurrences', 'lon', 'lat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import folium\n",
    "import numpy as np\n",
    "\n",
    "# Create a base map centered around Switzerland\n",
    "m = folium.Map(location=[46.8182, 8.2275], zoom_start=8)\n",
    "\n",
    "# Add city points to the map\n",
    "for index, row in cities_df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lon'], row['lat']),\n",
    "        radius=np.sqrt(row['no_occurrences'] / np.pi) * 5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.6,\n",
    "        tooltip=row['city'] + ': ' + str(row['no_occurrences']) + ' occurrences'\n",
    "    ).add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rts-bWoRmFur-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
