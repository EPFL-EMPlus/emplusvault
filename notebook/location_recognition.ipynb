{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing Locations from extracted text\n",
    "\n",
    "Different methods to get the location the RTS broadcasts are about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emv.db.dao import DataAccessObject\n",
    "from sqlalchemy.sql import text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import folium\n",
    "\n",
    "from emv.client.get_content import get_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = text(\"\"\"SELECT * FROM feature WHERE feature_type = 'transcript+ner';\"\"\")\n",
    "df = pd.DataFrame(DataAccessObject().fetch_all(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_entities = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for ent in row['data']['entities']:\n",
    "            if ent[1] == 'LOC':\n",
    "                loc_entities.append(ent[0].lower())\n",
    "                # print(ent['text'])\n",
    "        # print(row['data']['entities'])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series(loc_entities).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_locs = []\n",
    "for s in series.items():\n",
    "    all_locs.append(s)\n",
    "print(all_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match streets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = []\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for t in row['data']['transcript']:\n",
    "            streets.append(t['t'])\n",
    "        # row['data']['transcript'][0]['t']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        for t in row['data']['transcript']:\n",
    "            if 'rue de' in t['t'].lower():\n",
    "                print(t['t'])\n",
    "        # row['data']['transcript'][0]['t']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/geneva_streets.txt\", \"r\") as f:\n",
    "    streets = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_street(sent):\n",
    "    corrected_street = sent.replace(\"ruedes\", \"rue des\")\n",
    "    corrected_street = corrected_street.replace(\"cheminde\", \"chemin de\")\n",
    "    corrected_street = corrected_street.replace(\"placedes\", \"place des\")\n",
    "    corrected_street = corrected_street.replace(\"placede\", \"place de\")\n",
    "    corrected_street = corrected_street.replace(\"routedes\", \"route des\")\n",
    "    corrected_street = corrected_street.replace(\"avenuede\", \"avenue de\")\n",
    "    corrected_street = corrected_street.replace(\"avenuedu\", \"avenue du\")\n",
    "    corrected_street = corrected_street.replace(\"ruede\", \"rue de\")\n",
    "    corrected_street = corrected_street.replace(\"quaidu\", \"quai du\")\n",
    "    corrected_street = corrected_street.replace(\"placedu\", \"place du\")\n",
    "    corrected_street = corrected_street.replace(\"promenadedu\", \"promenade du\")\n",
    "    corrected_street = corrected_street.replace(\"ruedu\", \"rue du\")\n",
    "    corrected_street = corrected_street.replace(\"routede\", \"route de\")\n",
    "    corrected_street = corrected_street.replace(\"passagedes\", \"passage des\")\n",
    "    corrected_street = corrected_street.replace(\"chemindes\", \"chemin des\")\n",
    "    corrected_street = corrected_street.replace(\"ruedes\", \"rue des\")\n",
    "    corrected_street = corrected_street.replace(\"squaredu\", \"square du\")\n",
    "    corrected_street = corrected_street.replace(\"passagede\", \"passage de\")\n",
    "    corrected_street = corrected_street.replace(\"promenade des\", \"promenade des \")\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load the French language model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Initialize the Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "street_names = [\"Rue\", \"Chemin\", \"Place\", \"Avenue\", \"Boulevard\", \"Quai\", \"Promenade\", \"Route\", \"Square\"]\n",
    "connectors = [\"des\", \"de\", \"du\", \"la\", \"le\", \"les\", \"l'\", \"d'\", \"au\", \"aux\"]\n",
    "second_connectors = [\"l'\", \"d'\", \"la\"]\n",
    "\n",
    "# Define the pattern\n",
    "pattern = [\n",
    "    {\"TEXT\": {\"IN\": street_names}},\n",
    "    {\"TEXT\": {\"IN\": connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"REGEX\": \"^[a-zA-Z'-]+$\"}, \"OP\": \"+\"},\n",
    "]\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"ADDRESS\", [pattern])\n",
    "\n",
    "# create a second pattern to match things like Rue de l'Hôtel-de-Ville or Chemin de la Gravière\n",
    "pattern2 = [\n",
    "    {\"TEXT\": {\"IN\": street_names}},\n",
    "    {\"TEXT\": {\"IN\": connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"IN\": second_connectors}, \"OP\": \"?\"},\n",
    "    {\"TEXT\": {\"REGEX\": \"^[a-zA-Z-ôèéê]+$\"}, \"OP\": \"+\"},\n",
    "\n",
    "]\n",
    "matcher.add(\"ADDRESS2\", [pattern2])\n",
    "\n",
    "# Process the sentences and get the matches\n",
    "matched_streets = []\n",
    "\n",
    "for street in streets:\n",
    "    street = replace_street(street)\n",
    "    # sentence = f\"Hier, un accident s'est produit dans le sud de Genève, {street}, et 5 personnes ont été blessées. Le trafic est perturbé dans le secteur.\"\n",
    "    sentence = street\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    # if matches are overlapped, we only keep the longest one\n",
    "    if len(matches) > 1:\n",
    "        matches = sorted(matches, key=lambda x: x[2]-x[1], reverse=True)\n",
    "        matches = [matches[0]]\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        matched_streets.append(span.text)\n",
    "\n",
    "matched_streets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(feature_type='transcript+ner', max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.media_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(data):\n",
    "    entities = []\n",
    "    if \"entities\" in data.keys():\n",
    "        entities = data[\"entities\"]\n",
    "    else:\n",
    "        entities = [t.get(\"entities\", []) for t in data.get(\"transcript\", [])]\n",
    "        entities = [e for sublist in entities for e in sublist]\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entities\"] = df[\"data\"].apply(get_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"locations\"] = df[\"entities\"].apply(lambda x: [e[0] for e in x if e[1] == \"LOC\" and len(e[0]) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = df[\"locations\"].explode().value_counts()\n",
    "locations = pd.DataFrame(locations).reset_index().rename(columns={\"locations\":\"location\"})\n",
    "locations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../emv/features/cities.json\", \"r\") as f:\n",
    "    cities = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame([{\"location\":k, \"lon\":float(v[0]), \"lat\":float(v[1])} for k,v in cities.items() if len(v) == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_cities = pd.merge(locations, cities, on=\"location\", how=\"left\").dropna(subset = [\"lat\", \"lon\"])\n",
    "found_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base map centered around Switzerland\n",
    "m = folium.Map(location=[46.8182, 8.2275], zoom_start=8)\n",
    "size_multiplier = 1\n",
    "# Add city points to the map\n",
    "for index, row in found_cities.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=(row['lon'], row['lat']),\n",
    "        radius=np.sqrt(row['count'] / np.pi) * size_multiplier,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6,\n",
    "        tooltip=row['location'] + ': ' + str(row['count']) + ' occurrences'\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rts-bWoRmFur-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
