{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../..\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from rts.features.pose import compute_human_angles, reshape_keypoints\n",
    "import rts.features.pose as pose\n",
    "import umap\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From jsonlines files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_folder = \"data/test_poses/\"\n",
    "poses_jl = [poses_folder + f for f in os.listdir(poses_folder)]\n",
    "poses = []\n",
    "for pose_json in poses_jl:\n",
    "    extracted_data = pose.extract_frame_data(pose_json, 0.5)\n",
    "    if len(extracted_data.keys()) > 0:\n",
    "        pose_exp = [[{\"frame_number\":k, \"angle_vec\":angle, \"keypoints\":keypoint, \"bbox\":bbox} for angle,keypoint,bbox in zip(v[\"angle_vec\"], v[\"keypoints\"], v[\"bbox\"])] for k,v in extracted_data.items()]\n",
    "        pose_exp = [item for sublist in pose_exp for item in sublist]\n",
    "        [p.update({\"video_name\":pose_json.split(\"/\")[-1].split(\".\")[0]}) for p in pose_exp]\n",
    "        poses.extend(pose_exp)\n",
    "\n",
    "print(len(poses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINTS_NAMES = [\"nose\", \"left_eye\", \"right_eye\", \"left_ear\", \"right_ear\", \n",
    "                   \"left_shoulder\", \"right_shoulder\", \"left_elbow\", \"right_elbow\", \n",
    "                   \"left_wrist\", \"right_wrist\", \"left_hip\", \"right_hip\", \n",
    "                   \"left_knee\", \"right_knee\", \"left_ankle\", \"right_ankle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTIONS = [\n",
    "    (\"nose\", \"left_eye\"),\n",
    "    (\"nose\", \"right_eye\"),\n",
    "    (\"left_eye\", \"left_ear\"),\n",
    "    (\"right_eye\", \"right_ear\"),\n",
    "    (\"left_shoulder\", \"right_shoulder\"),\n",
    "    (\"left_shoulder\", \"left_elbow\"),\n",
    "    (\"right_shoulder\", \"right_elbow\"),\n",
    "    (\"left_elbow\", \"left_wrist\"),\n",
    "    (\"right_elbow\", \"right_wrist\"),\n",
    "    (\"left_hip\", \"right_hip\"),\n",
    "    (\"left_hip\", \"left_knee\"),\n",
    "    (\"right_hip\", \"right_knee\"),\n",
    "    (\"left_knee\", \"left_ankle\"),\n",
    "    (\"right_knee\", \"right_ankle\"),\n",
    "    (\"left_shoulder\", \"left_hip\"),\n",
    "    (\"right_shoulder\", \"right_hip\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_keypoints_to_read(keypoints):\n",
    "    return {k:v for k,v in zip(KEYPOINTS_NAMES, keypoints)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_keypoints_to_read(poses[3][\"keypoints\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_folder = \"/mnt/g/ioc/sequences/\"\n",
    "\n",
    "def get_frame(video_name, frame_number):\n",
    "    video_path = [f for f in os.listdir(sequences_folder + video_name) if f.endswith(\".mp4\")][0]\n",
    "    cap = cv2.VideoCapture(sequences_folder + video_name + \"/\" + video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pose(pose, ax = None, cut=True, threshold=0.1):\n",
    "    keypoints = pose[\"keypoints\"]\n",
    "\n",
    "    # Do not draw non-detected keypoints\n",
    "    #connections = [c for c in connections if keypoints[KEYPOINTS_NAMES.index(c[0])][2] > threshold and keypoints[KEYPOINTS_NAMES.index(c[1])][2] > threshold]\n",
    "    #keypoints = [k for k in keypoints if k[2] > threshold]\n",
    "    \n",
    "    frame = get_frame(pose[\"video_name\"], pose[\"frame_number\"])\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "    ax.imshow(frame)\n",
    "    ax.scatter([k[0] for k in keypoints if k[2] > threshold], [k[1] for k in keypoints if k[2] > threshold], s=10)\n",
    "    for c in CONNECTIONS:\n",
    "        k1 = keypoints[KEYPOINTS_NAMES.index(c[0])]\n",
    "        k2 = keypoints[KEYPOINTS_NAMES.index(c[1])]\n",
    "        if k1[2] > threshold and k2[2] > threshold:\n",
    "            ax.plot([k1[0], k2[0]], \n",
    "                    [k1[1], k2[1]], \n",
    "                    linewidth=1, color='black')\n",
    "        \n",
    "    # cut frame to bbox\n",
    "    bbox = pose[\"bbox\"]\n",
    "    if cut:\n",
    "        ax.set_xlim(int(bbox[0]),int(bbox[0] + bbox[2]))\n",
    "        ax.set_ylim(int(bbox[1] + bbox[3]), int(bbox[1]))\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid of subplots\n",
    "nrows = 3\n",
    "ncols = 6\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 3, nrows * 3))\n",
    "axs = axs.flatten()\n",
    "# iterate over the poses and draw each pose in a subplot\n",
    "sample_poses = np.random.choice(poses, nrows * ncols)\n",
    "for i, pose in enumerate(sample_poses):\n",
    "    draw_pose(pose, ax = axs[i], cut = True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rts.utils import dataframe_from_hdf5\n",
    "\n",
    "archiva_path = \"/mnt/g/ioc/data/\"\n",
    "data = dataframe_from_hdf5(archiva_path, \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses_df = pd.DataFrame(poses)\n",
    "poses_df = pd.merge(poses_df, data[[\"seq_id\", \"sport\"]], left_on=\"video_name\", right_on=\"seq_id\")\n",
    "poses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sports = poses_df.groupby(\"sport\").count().sort_values(\"video_name\", ascending=False).head(10).index.tolist()\n",
    "top_sports_df = poses_df[poses_df[\"sport\"].isin(top_sports)]\n",
    "top_sports_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=1000, min_dist=0.3, metric='cosine')\n",
    "embedding = reducer.fit_transform(top_sports_df[\"angle_vec\"].tolist())\n",
    "top_sports_df[\"umap_x\"] = embedding[:,0]\n",
    "top_sports_df[\"umap_y\"] = embedding[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for sport in top_sports_df[\"sport\"].unique():\n",
    "    plt.scatter(top_sports_df[top_sports_df[\"sport\"] == sport][\"umap_x\"], \n",
    "                top_sports_df[top_sports_df[\"sport\"] == sport][\"umap_y\"], \n",
    "                s= 1, label = sport)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.legend(markerscale = 5)\n",
    "plt.title('UMAP projection of the pose angles', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, ImageURL\n",
    "from bokeh.io import push_notebook\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_binary = f.read()\n",
    "    return \"data:image/png;base64,\" + base64.b64encode(image_binary).decode()\n",
    "\n",
    "def create_annotation_image(pose):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    draw_pose(pose, ax=ax, cut=True)\n",
    "    tmp_path = \"data/test_images/tmp.png\"\n",
    "    plt.savefig(tmp_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return image_to_base64(tmp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POSES = 1000\n",
    "EVERY_N = 5\n",
    "embedded_images = [create_annotation_image(pose) for pose in poses[::EVERY_N][:N_POSES]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=int(0.1 * N_POSES), min_dist=0.9, metric='cosine')\n",
    "embedding = reducer.fit_transform([p[\"angle_vec\"] for p in poses[::EVERY_N][:N_POSES]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.io import push_notebook, export_png\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Example 2D points\n",
    "x = embedding[:, 0]\n",
    "y = embedding[:, 1]\n",
    "\n",
    "# Create a ColumnDataSource\n",
    "source = ColumnDataSource(data=dict(x=x, y=y, url=embedded_images))\n",
    "\n",
    "# Output to notebook\n",
    "output_notebook()\n",
    "\n",
    "# Create a new plot\n",
    "p = figure(width=1000, height=1000)\n",
    "\n",
    "\n",
    "# Add images\n",
    "p.image_url(url='url', x='x', y='y', source=source, w=0.3, h=0.3, anchor=\"center\")\n",
    "\n",
    "# Show the plot\n",
    "handle = show(p, notebook_handle=True)\n",
    "\n",
    "# Save the plot\n",
    "export_png(p, filename='data/plt_test.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rts-bWoRmFur-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
