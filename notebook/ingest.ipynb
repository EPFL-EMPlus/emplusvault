{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kikohs/work/rts\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "RTS_DATA = \"/mnt/d/rts/\"\n",
    "METADATA = RTS_DATA + 'metadata'\n",
    "VIDEOS = RTS_DATA + '0'\n",
    "REMOTE_VIDEOS = \"/mnt/rts\"\n",
    "OUTDIR = 'data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import meilisearch\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import io\n",
    "from typing import Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL imports\n",
    "import rts\n",
    "from rts.ingest import read_video_folder_index\n",
    "\n",
    "LOG = rts.utils.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidx = read_video_folder_index(os.path.join(OUTDIR, 'vidx.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meilisearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client \u001b[39m=\u001b[39m meilisearch\u001b[39m.\u001b[39mClient(\u001b[39m'\u001b[39m\u001b[39mhttp://localhost:7700\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1234\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39m# client.get_keys()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m  \n\u001b[1;32m      4\u001b[0m \u001b[39m# int(datetime.now().timestamp())\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# client.create_index('books', {'primaryKey': 'reference_number'})\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meilisearch' is not defined"
     ]
    }
   ],
   "source": [
    "client = meilisearch.Client('http://localhost:7700', '1234')\n",
    "# client.get_keys()\n",
    " \n",
    "# int(datetime.now().timestamp())\n",
    "# client.create_index('books', {'primaryKey': 'reference_number'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:RTS:'idSupport'\n",
      "{'CategorieAsset': 'Programme', 'SemaphoreDroit': 'Inconnu', 'DureeSec': 3821, 'DateModification': '2018-04-17T01:40:48Z', 'HeurePublication': '09:39:00', 'idCollection': '408', 'ApplicationModification': 'TMS', 'NumeroEpisode': 101, 'idTypeProduction': '6', 'Collection': 'Ski alpin', 'idTypeAsset': '5', 'idStatutArchivage': '3', 'idCategorieAsset': 'PROGR', 'aVignette': False, 'DatePublication': '2011-10-22T09:39:00Z', 'DateCreation': '2011-09-29T12:12:26Z', 'Guid': 'AA1109014960', 'TypeProduction': 'Transmission', 'key': 'asset-14961', 'StatutArchivage': 'Terminé', 'Titre': 'Géant dames, 1re manche, Sölden', 'environment': 'production', 'ApplicationCreation': 'STRADA', 'batchid': 1645384501, 'role': 'asset', 'TypeAsset': 'Retransmission sportive', 'Duree': '01:03:41:00', 'idTypeContenu': ['Multiple'], 'idCanalPublication': ['588'], 'id': '14961', 'UtilisateurCreation': 'STRADA', 'Saison': '2011', 'CodeCollection': 'SKIAL', 'TypeContenu': ['Valeurs multiples'], 'idSemaphoreDroit': 'G', 'CanalPublication': ['TSR 2'], 'version': '1.13.4', 'UtilisateurModification': 'TMS', 'Achat': 'Non', 'aTexts': True, 'MetadonneesAutomatiques': 'En direct de Sölden (AUT)', 'timestamp': '2022-02-21T00:53:41.170Z'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 41\u001b[0m, in \u001b[0;36mparse_item\u001b[0;34m(raw, vidx)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     seq_idx, media_id \u001b[39m=\u001b[39m find_media_id(d[\u001b[39m'\u001b[39;49m\u001b[39midSupport\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     42\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'idSupport'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 128\u001b[0m\n\u001b[1;32m    123\u001b[0m         all_d\u001b[39m.\u001b[39mupdate(d)\n\u001b[1;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m all_d\n\u001b[0;32m--> 128\u001b[0m medias \u001b[39m=\u001b[39m read_metadata_zippart(METADATA, \u001b[39m0\u001b[39;49m, vidx)\n",
      "Cell \u001b[0;32mIn[21], line 111\u001b[0m, in \u001b[0;36mread_metadata_zippart\u001b[0;34m(root_dir, part_num, vidx)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mwith\u001b[39;00m j\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m    110\u001b[0m     js \u001b[39m=\u001b[39m orjson\u001b[39m.\u001b[39mloads(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m--> 111\u001b[0m     media \u001b[39m=\u001b[39m parse_item(js, vidx)\n\u001b[1;32m    112\u001b[0m     \u001b[39m# ts = read_txt_transcript(srts, media)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39m# media['ts'] = ts\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     medias[media[\u001b[39m'\u001b[39m\u001b[39mguid\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m media\n",
      "Cell \u001b[0;32mIn[21], line 45\u001b[0m, in \u001b[0;36mparse_item\u001b[0;34m(raw, vidx)\u001b[0m\n\u001b[1;32m     43\u001b[0m     LOG\u001b[39m.\u001b[39merror(e)\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(d)\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m\n\u001b[1;32m     48\u001b[0m duration \u001b[39m=\u001b[39m d[\u001b[39m'\u001b[39m\u001b[39mDureeMediaSec\u001b[39m\u001b[39m'\u001b[39m][seq_idx]\n\u001b[1;32m     49\u001b[0m ratio \u001b[39m=\u001b[39m d[\u001b[39m'\u001b[39m\u001b[39mRatioMedia\u001b[39m\u001b[39m'\u001b[39m][seq_idx]\n",
      "\u001b[0;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def parse_item_sequences(data: Dict, media: Dict) -> Dict:\n",
    "    seqs = {}\n",
    "    for id in data.get('idSequence', []):\n",
    "        seq = {\n",
    "            'guid': media['guid'],\n",
    "            'mediaId': media['mediaId'],\n",
    "            'seqid': id,\n",
    "            'geo': [],\n",
    "            'mat': [],\n",
    "            'pp': []\n",
    "        }\n",
    "        seqs[seq['seqid']] = seq\n",
    "\n",
    "    geo = data.get('VisuelsGEOSequence', [])\n",
    "    for g in geo:\n",
    "        seqid, loc = g.split('@')\n",
    "        seqs[seqid]['geo'].append(loc.lower())\n",
    "\n",
    "    mat = data.get('VisuelsMATSequence', [])\n",
    "    for m in mat:\n",
    "        seqid, mat = m.split('@')\n",
    "        seqs[seqid]['mat'].append(mat.lower())\n",
    "    \n",
    "    pp = data.get('VisuelsPPSequence', [])\n",
    "    for p in pp:\n",
    "        seqid, name = p.split('@')\n",
    "        seqs[seqid]['pp'].append(name.lower())\n",
    "    \n",
    "    return seqs\n",
    "\n",
    "\n",
    "def parse_item(raw: Dict, vidx: Dict) -> Optional[Dict]:\n",
    "    def find_media_id(support: Dict) -> Tuple[Optional[int], Optional[str]]:\n",
    "        for i, d in enumerate(support):\n",
    "            if d.startswith('Z'):\n",
    "                return i, d\n",
    "        return None, None\n",
    "\n",
    "    d = raw['response']['docs'][0]\n",
    "    try:\n",
    "        seq_idx, media_id = find_media_id(d['idSupport'])\n",
    "    except KeyError as e:\n",
    "        LOG.error(e)\n",
    "        return None\n",
    "\n",
    "    duration = d['DureeMediaSec'][seq_idx]\n",
    "    ratio = d['RatioMedia'][seq_idx]\n",
    "\n",
    "    media = {\n",
    "        'guid': d['Guid'],\n",
    "        'mediaId': media_id,\n",
    "        'mediaFolderPath': vidx.get(media_id),\n",
    "        'mediaDuration': duration,\n",
    "        'ratio': ratio,\n",
    "        'formatType': d['FormatMedia'][seq_idx],\n",
    "        'formatResolution': d['DefinitionMedia'][seq_idx],\n",
    "        'materialType': d['TypeMaterielMedia'][seq_idx],\n",
    "        'publishedDate': d.get('DatePublication'),\n",
    "        'publishedBy': d.get('CanalPublication'),\n",
    "\n",
    "        'rights': d.get('SemaphoreDroit'),\n",
    "        'categoryName': d.get('CategorieAsset'),\n",
    "        'assetType': d.get('TypeAsset'),\n",
    "        'contentType': d.get('TypeContenu'),\n",
    "        'backgoundType': d.get('idTypeBackground'),\n",
    "        'collection': d.get('Collection'),\n",
    "        \n",
    "        'title': d.get('Titre'),\n",
    "        'resume': d.get('Resume'),\n",
    "        'geoTheme': d.get('ThematiquesGEO'),\n",
    "        'resumeSequence': d.get('ResumeSequence'),\n",
    "        'created': int(datetime.now().timestamp()),\n",
    "        'published': int(datetime.strptime(d.get('DatePublication'), \"%Y-%m-%dT%H:%M:%S%z\").timestamp())\n",
    "    }\n",
    "\n",
    "    seqs = parse_item_sequences(d, media)\n",
    "    media['sequences'] = seqs\n",
    "\n",
    "    return media\n",
    "\n",
    "\n",
    "def read_txt_transcript(srtz: zipfile.Path, media: Dict) -> Optional[Dict]:\n",
    "    try:\n",
    "        ts = {\n",
    "            'guid': media['guid'],\n",
    "            'mediaId': media['mediaId'],\n",
    "            'version': 1,\n",
    "            'text': ''\n",
    "        }\n",
    "        path = media['guid'] + '_0.txt'\n",
    "        current = srtz.joinpath(path)\n",
    "        with current.open('r') as fp:\n",
    "            wrap = io.TextIOWrapper(fp, encoding='latin-1')\n",
    "            ts['text'] = wrap.read()\n",
    "    except Exception as e:\n",
    "        LOG.error('Read txt ts:', e)\n",
    "        return None\n",
    "    return ts\n",
    "\n",
    "\n",
    "def read_metadata_zippart(root_dir: str, part_num: int, vidx: Dict) -> Dict:\n",
    "    path = os.path.join(root_dir, f'{part_num}.zip')\n",
    "    medias = {}\n",
    "    with zipfile.ZipFile(path, 'r') as z:\n",
    "        jsons = zipfile.Path(z, f'{part_num}/jsonData/')\n",
    "        for j in jsons.iterdir():\n",
    "            with j.open('r') as fp:\n",
    "                js = orjson.loads(fp.read())\n",
    "                media = parse_item(js, vidx)\n",
    "                if media:\n",
    "                # ts = read_txt_transcript(srts, media)\n",
    "                # media['ts'] = ts\n",
    "                    medias[media['guid']] = media\n",
    "\n",
    "    return medias\n",
    "\n",
    "\n",
    "def read_all_metadata(root_dir: str, vidx: Dict) -> Dict:\n",
    "    all_d = None\n",
    "    for i in range(0, 10):\n",
    "        d = read_metadata_zippart(root_dir, i, vidx)\n",
    "        all_d.update(d)\n",
    "    \n",
    "    return all_d\n",
    "\n",
    "\n",
    "medias = read_metadata_zippart(METADATA, 0, vidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "# result = model.transcribe(\"audio.mp3\")\n",
    "# print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vidx = get_video_folder_index(REMOTE_VIDEOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# vidx_tojson(vidx, '/home/kikohs/work/rts/data/vidx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kikohs'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/vidx.json'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(OUTDIR, 'vidx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2 = read_vidx('/home/kikohs/work/rts/data/vidx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('rts-DIiCth0j-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3302d3dcc6e58bb01d2919129e3db2b24bfcd2253c5885c52ed63a4927bacf6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
